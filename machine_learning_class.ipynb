{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ade6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75db9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diário: (411144, 29)\n",
      "Semanal: (85971, 29)\n",
      "Mensal: (19802, 29)\n",
      "\n",
      "Tipo da coluna 'datetime' (Diário): datetime64[ns]\n",
      "\n",
      "Amostra Dados Diários:\n",
      "    datetime     close      high       low      open  volume    ticker  EMA_9  \\\n",
      "0 2000-01-05  0.248809  0.248809  0.248809  0.248809     985  ABEV3.SA    NaN   \n",
      "1 2000-01-06  0.236196  0.236196  0.236196  0.236196     227  ABEV3.SA    NaN   \n",
      "2 2000-01-07  0.236196  0.236196  0.236196  0.236196     151  ABEV3.SA    NaN   \n",
      "3 2000-01-10  0.236196  0.236196  0.236196  0.236196    1516  ABEV3.SA    NaN   \n",
      "4 2000-01-11  0.236196  0.236196  0.236196  0.236196    3791  ABEV3.SA    NaN   \n",
      "\n",
      "   SMA_21  SMA_50  ...  BBP_20_2.0_2.0  STOCHk_14_3_3  STOCHd_14_3_3  \\\n",
      "0     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "1     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "2     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "3     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "4     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "\n",
      "   STOCHh_14_3_3    OBV  ATRr_14  ADX_14  ADXR_14_2  DMP_14  DMN_14  \n",
      "0            NaN    NaN      NaN     NaN        NaN     NaN     NaN  \n",
      "1            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "2            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "3            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "4            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Conectar ao banco de dados SQLite\n",
    "DB_FILE = \"dados_acoes.db\"\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "\n",
    "# Carregar as tabelas\n",
    "# O usuário confirmou que as colunas 'date' já estão em formato datetime\n",
    "try:\n",
    "    df_diario = pd.read_sql_query(\"SELECT * FROM diario_com_indicadores\", conn)\n",
    "    df_semanal = pd.read_sql_query(\"SELECT * FROM semanal_com_indicadores\", conn)\n",
    "    df_mensal = pd.read_sql_query(\"SELECT * FROM mensal_com_indicadores\", conn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Verificação extra: garantir que o pandas as reconheceu como datetime\n",
    "    # Se elas forem strings, o merge_asof falhará.\n",
    "    df_diario['datetime'] = pd.to_datetime(df_diario['datetime'])\n",
    "    df_semanal['datetime'] = pd.to_datetime(df_semanal['datetime'])\n",
    "    df_mensal['datetime'] = pd.to_datetime(df_mensal['datetime'])\n",
    "\n",
    "    print(f\"Diário: {df_diario.shape}\")\n",
    "    print(f\"Semanal: {df_semanal.shape}\")\n",
    "    print(f\"Mensal: {df_mensal.shape}\")\n",
    "    \n",
    "    print(\"\\nTipo da coluna 'datetime' (Diário):\", df_diario['datetime'].dtype)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    print(\"Verifique se o nome do arquivo 'dados_acoes.db' e os nomes das tabelas estão corretos.\")\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# Visualizar os dados\n",
    "print(\"\\nAmostra Dados Diários:\")\n",
    "print(df_diario.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amostra de Dados Unificados:\n",
      "    datetime       close        high         low        open    volume  \\\n",
      "0 2000-01-03    0.444428    0.463940    0.444428    0.455268   1029600   \n",
      "1 2000-01-03   53.865044   53.865044   53.865044   53.865044       145   \n",
      "2 2000-01-03    0.327688    0.345801    0.306281    0.307928  13152318   \n",
      "3 2000-01-03  129.651276  131.126621  128.161031  129.651276    150000   \n",
      "4 2000-01-03  214.932358  217.591315  212.716560  213.159720   1210000   \n",
      "\n",
      "     ticker      EMA_9     SMA_21     SMA_50  ...  BBP_20_2.0_2.0_men  \\\n",
      "0  BBAS3.SA  10.141362  12.038306  12.614476  ...           -0.404727   \n",
      "1  LIGT3.SA  24.982800  19.414526  18.789501  ...            1.552200   \n",
      "2  ITSA4.SA  38.178534  45.309890  46.331954  ...           -0.385870   \n",
      "3  GOAU4.SA  30.972806  12.267680   9.200727  ...            1.561928   \n",
      "4  GGBR4.SA  54.807740  24.783922  19.350447  ...            1.561622   \n",
      "\n",
      "   STOCHk_14_3_3_men  STOCHd_14_3_3_men  STOCHh_14_3_3_men       OBV_men  \\\n",
      "0          53.698555          65.250575         -11.552019 -9.521600e+09   \n",
      "1          62.569316          38.593369          23.975947 -4.125575e+11   \n",
      "2          51.173148          63.283140         -12.109992 -4.771428e+11   \n",
      "3          71.016681          62.006443           9.010239 -4.791943e+11   \n",
      "4          90.755441          87.918217           2.837224 -4.812348e+11   \n",
      "\n",
      "   ATRr_14_men  ADX_14_men  ADXR_14_2_men  DMP_14_men  DMN_14_men  \n",
      "0     3.312594   20.812228      18.824192   11.536864   30.955799  \n",
      "1     6.124019   22.479420      20.094992   60.270481   24.294587  \n",
      "2    15.464539   17.066745      15.226792   12.460076   29.143012  \n",
      "3    11.316480   14.718549      10.508759   87.158421    5.986637  \n",
      "4    19.765694   22.056788      18.280244   89.403677    6.186510  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "Shape após merge: (411144, 83)\n"
     ]
    }
   ],
   "source": [
    "# Garantir que tudo está ordenado por data para o merge_asof funcionar\n",
    "df_diario = df_diario.sort_values(by='datetime')\n",
    "df_semanal = df_semanal.sort_values(by='datetime')\n",
    "df_mensal = df_mensal.sort_values(by='datetime')\n",
    "\n",
    "# Renomear colunas de indicadores para evitar conflitos (ex: 'RSI' diário, 'RSI' semanal)\n",
    "df_semanal = df_semanal.add_suffix('_sem')\n",
    "df_mensal = df_mensal.add_suffix('_men')\n",
    "\n",
    "# Renomear colunas de junção\n",
    "df_semanal = df_semanal.rename(columns={'datetime_sem': 'datetime', 'ticker_sem': 'ticker'})\n",
    "df_mensal = df_mensal.rename(columns={'datetime_men': 'datetime', 'ticker_men': 'ticker'})\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Junção (Merge)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Juntar Diário com Semanal\n",
    "# Para cada 'ticker', vamos juntar a data diária com a data semanal mais próxima (anterior ou igual)\n",
    "df_merged = pd.merge_asof(\n",
    "    df_diario,\n",
    "    df_semanal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward' # 'backward' pega o último dado semanal disponível para aquele dia\n",
    ")\n",
    "\n",
    "# 2. Juntar o resultado com o Mensal\n",
    "df_merged = pd.merge_asof(\n",
    "    df_merged,\n",
    "    df_mensal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "print(\"\\nAmostra de Dados Unificados:\")\n",
    "print(df_merged.head())\n",
    "\n",
    "print(f\"\\nShape após merge: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981b6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definição do Alvo (y) ---\n",
    "PERIOD_HORIZON = 30\n",
    "TIME_STEPS = 30 # Hiperparâmetro: quantos dias o LSTM vai \"olhar para trás\"\n",
    "\n",
    "# <-- MUDANÇA: Threshold para classificação binária (0 = retorno > 0%)\n",
    "CLASSIFICATION_THRESHOLD = 0.05\n",
    "\n",
    "# <-- MUDANÇA: Novo nome de arquivo para o modelo de classificação\n",
    "MODEL_FILE = \"transformers_stock_model_30d_ts30_CLASS_weighted.keras\" # Nome do arquivo para salvar/carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b2d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape final após limpeza e 'y_target': (395678, 86)\n",
      "Distribuição do Alvo (y_target):\n",
      "y_target\n",
      "0    0.639947\n",
      "1    0.360053\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Usando 81 features:\n",
      "['close', 'high', 'low', 'open', 'volume', 'EMA_9', 'SMA_21', 'SMA_50', 'SMA_200', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_20_2.0_2.0', 'BBM_20_2.0_2.0', 'BBU_20_2.0_2.0', 'BBB_20_2.0_2.0', 'BBP_20_2.0_2.0', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'STOCHh_14_3_3', 'OBV', 'ATRr_14', 'ADX_14', 'ADXR_14_2', 'DMP_14', 'DMN_14', 'close_sem', 'high_sem', 'low_sem', 'open_sem', 'volume_sem', 'EMA_9_sem', 'SMA_21_sem', 'SMA_50_sem', 'SMA_200_sem', 'RSI_14_sem', 'MACD_12_26_9_sem', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'BBL_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'BBU_20_2.0_2.0_sem', 'BBB_20_2.0_2.0_sem', 'BBP_20_2.0_2.0_sem', 'STOCHk_14_3_3_sem', 'STOCHd_14_3_3_sem', 'STOCHh_14_3_3_sem', 'OBV_sem', 'ATRr_14_sem', 'ADX_14_sem', 'ADXR_14_2_sem', 'DMP_14_sem', 'DMN_14_sem', 'close_men', 'high_men', 'low_men', 'open_men', 'volume_men', 'EMA_9_men', 'SMA_21_men', 'SMA_50_men', 'SMA_200_men', 'RSI_14_men', 'MACD_12_26_9_men', 'MACDh_12_26_9_men', 'MACDs_12_26_9_men', 'BBL_20_2.0_2.0_men', 'BBM_20_2.0_2.0_men', 'BBU_20_2.0_2.0_men', 'BBB_20_2.0_2.0_men', 'BBP_20_2.0_2.0_men', 'STOCHk_14_3_3_men', 'STOCHd_14_3_3_men', 'STOCHh_14_3_3_men', 'OBV_men', 'ATRr_14_men', 'ADX_14_men', 'ADXR_14_2_men', 'DMP_14_men', 'DMN_14_men']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por ticker para calcular o shift corretamente\n",
    "df_merged['close_future'] = df_merged.groupby('ticker')['close'].shift(-PERIOD_HORIZON)\n",
    "\n",
    "\n",
    "# 1. Calcular o retorno futuro (para referência e para criar o alvo)\n",
    "df_merged['y_return'] = (df_merged['close_future'] / df_merged['close']) - 1\n",
    "\n",
    "# 2. Criar o alvo de CLASSIFICAÇÃO (1 se o retorno > threshold, 0 caso contrário)\n",
    "df_merged['y_target'] = (df_merged['y_return'] > CLASSIFICATION_THRESHOLD).astype(int)\n",
    "\n",
    "# --- Limpeza ---\n",
    "# Remover dados onde não pudemos calcular o alvo (os últimos N dias de cada ticker)\n",
    "# Também remover quaisquer NaNs gerados pelos merges ou cálculos de indicadores\n",
    "df_final = df_merged.dropna()\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final está vazio após o dropna().\")\n",
    "    print(\"Verifique seus dados de entrada, a lógica de merge e o cálculo do 'y_target'.\")\n",
    "else:\n",
    "    print(f\"\\nShape final após limpeza e 'y_target': {df_final.shape}\")\n",
    "    print(f\"Distribuição do Alvo (y_target):\\n{df_final['y_target'].value_counts(normalize=True)}\")\n",
    "\n",
    "    # --- Definição das Features (X) ---\n",
    "    features_diarias = [col for col in df_final.columns if col not in ['datetime', 'ticker'] and not col.endswith('_sem') and not col.endswith('_men')]\n",
    "    features_semanais = [col for col in df_final.columns if col.endswith('_sem') and col not in ['date_sem', 'ticker_sem']]\n",
    "    features_mensais = [col for col in df_final.columns if col.endswith('_men') and col not in ['date_men', 'ticker_men']]\n",
    "    all_features = features_diarias + features_semanais + features_mensais\n",
    "    \n",
    "    # Defina as colunas que NUNCA devem ser features\n",
    "    # <-- MUDANÇA: Adicionado 'y_return' à exclusão\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    \n",
    "    # Filtra para garantir que são numéricas E não são as colunas de exclusão\n",
    "    features_numericas = [col for col in all_features if pd.api.types.is_numeric_dtype(df_final[col])]\n",
    "    features = [col for col in features_numericas if col not in colunas_a_excluir]\n",
    "    print(f\"\\nUsando {len(features)} features:\")\n",
    "    print(features)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO !!!\")\n",
    "        print(f\"As seguintes colunas-alvo ESTÃO na sua lista 'features' (X): {leaky_cols}\")\n",
    "        print(f\"Corrija sua lista 'colunas_a_excluir' na Célula [5].\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    else:\n",
    "        print(\">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7465383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (253233, 81)\n",
      "Shape Validação 2D (X_validation): (63309, 81)\n",
      "Shape Teste 2D (X_test): (79136, 81)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2019-12-12 00:00:00\n",
      "Datas de Validação: 2019-12-12 00:00:00 a 2022-09-05 00:00:00\n",
      "Datas de Teste: 2022-09-05 00:00:00 a 2025-09-16 00:00:00\n",
      "\n",
      "X_train_scaled shape: (253233, 81)\n",
      "X_val_scaled shape: (63309, 81)\n",
      "X_test_scaled shape: (79136, 81)\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 2 - Split Treino/Validação/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+validação\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Validação (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Validação 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Validação: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normalização (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os três sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a função de sequência)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5639e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequências de TREINO...\n",
      "Criando sequências de VALIDAÇÃO...\n",
      "Criando sequências de TESTE...\n",
      "\n",
      "Formato das Sequências de Treino (X): (250884, 30, 81)\n",
      "Formato dos Alvos de Treino (y): (250884,)\n",
      "\n",
      "Formato das Sequências de Validação (X): (60209, 30, 81)\n",
      "Formato dos Alvos de Validação (y): (60209,)\n",
      "\n",
      "Formato das Sequências de Teste (X): (75896, 30, 81)\n",
      "Formato dos Alvos de Teste (y): (75896,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequências de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequências não cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os índices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker específico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O índice original é mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele será ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequência de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # Índice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDANÇA: Solução 2 - Criar Sequências para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequências de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de VALIDAÇÃO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta é a variável que a Célula [9] (Avaliação) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequências de Validação (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Validação (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste não encontrados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c230f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de Classe Calculados: {np.int64(0): np.float64(0.7946659909410535), np.int64(1): np.float64(1.348418235173978)}\n",
      "Reloading Tuner from keras_tuner_dir\\stock_TRANSFORMER_tuning_weighted_rocauc\\tuner0.json\n",
      "--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\n",
      "--- BUSCA CONCLUÍDA ---\n",
      "\n",
      "    Melhores hiperparâmetros encontrados:\n",
      "    - num_blocks: 1\n",
      "    - num_heads: 4\n",
      "    - head_size: 64\n",
      "    - ff_dim: 324\n",
      "    - dropout: 0.20\n",
      "    - dense_units: 32\n",
      "    - learning_rate: 0.00202\n",
      "    \n",
      "WARNING:tensorflow:From c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Melhor modelo (Transformer) carregado na variável 'model_transformer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_Tuner_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Transformer_Tuner_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Input_Sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input_Sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">83,793</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">162</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">324</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,568</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,325</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">162</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Global_Pooling      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Global_Pooling[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense_Classifier    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ Dense_Classifier… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Input_Sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ Input_Sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │     \u001b[38;5;34m83,793\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │        \u001b[38;5;34m162\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m324\u001b[0m)   │     \u001b[38;5;34m26,568\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │     \u001b[38;5;34m26,325\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m81\u001b[0m)    │        \u001b[38;5;34m162\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Global_Pooling      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ Global_Pooling[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense_Classifier    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,624\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ Dense_Classifier… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,667</span> (545.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139,667\u001b[0m (545.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,667</span> (545.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139,667\u001b[0m (545.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando o melhor modelo em 'transformers_stock_model_30d_ts30_CLASS_weighted.keras'...\n",
      "Modelo salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um único bloco Transformer Encoder.\n",
    "    d_model = dimensão da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta atenção\" a diferentes partes da sequência de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimensão original\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Função Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Função construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimensões dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimensão do modelo é o número de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro não sabe a *ordem* dos dias (é permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprendível\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posições (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posições (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posição aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperparâmetros para Tunar ---\n",
    "    \n",
    "    # 1. Número de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Parâmetros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabeças\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimensão da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regularização)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=5e-4, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que você tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constrói a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabeça de Classificação (Classification Head) ---\n",
    "    \n",
    "    # A saída 'x' ainda é uma sequência (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um único vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a média dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classificação\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classificação (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[AUC(name='roc_auc', curve='ROC'), 'accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configuração do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes únicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores às classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicionário de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq não encontrado. Pulando cálculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados não estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective=kt.Objective(\"val_roc_auc\", direction=\"max\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted_rocauc'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLUÍDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperparâmetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperparâmetros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Pegar o melhor modelo\n",
    "    model_transformer = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    print(\"Melhor modelo (Transformer) carregado na variável 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # Salvar o melhor modelo\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' não foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2de976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\n",
      "Acurácia (Accuracy) no Teste: 53.65%\n",
      "ROC AUC no Teste: 0.5163\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[29108 21148]\n",
      " [14033 11607]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BAIXA (0)       0.67      0.58      0.62     50256\n",
      "    ALTA (1)       0.35      0.45      0.40     25640\n",
      "\n",
      "    accuracy                           0.54     75896\n",
      "   macro avg       0.51      0.52      0.51     75896\n",
      "weighted avg       0.57      0.54      0.55     75896\n",
      "\n",
      "\n",
      "Amostra dos Resultados (LSTM):\n",
      "             date    ticker  y_real  y_pred_proba  y_pred_class\n",
      "331912 2022-10-19  BBAS3.SA       0      0.634730             1\n",
      "331940 2022-10-20  BBAS3.SA       0      0.637747             1\n",
      "332045 2022-10-21  BBAS3.SA       0      0.652116             1\n",
      "332183 2022-10-24  BBAS3.SA       0      0.660584             1\n",
      "332297 2022-10-25  BBAS3.SA       0      0.666610             1\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Avaliação de Classificação\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previsões no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # 3. Buscar as informações originais (data, ticker) usando os índices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avaliação de Métricas de Classificação\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcurácia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # --- CÁLCULO DE ROC AUC ADICIONADO ---\n",
    "    # (Usa as probabilidades, não as classes)\n",
    "    roc_auc = roc_auc_score(df_results['y_real'], df_results['y_pred_proba'])\n",
    "    print(f\"ROC AUC no Teste: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (LSTM):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b268dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\n",
      "Usando modelo treinado para prever a PROBABILIDADE de alta em 30 dias.\n",
      "Usando os últimos 30 dias de dados como entrada.\n",
      "==================================================\n",
      "\n",
      "Gerando previsões para 108 tickers...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\n",
      "        ticker last_data_date  predicted_proba_ALTA\n",
      "95    CSED3.SA     2025-10-28              0.947590\n",
      "43    MBRF3.SA     2025-10-28              0.947332\n",
      "82    AMBP3.SA     2025-10-28              0.939871\n",
      "12    CPLE6.SA     2025-10-28              0.914540\n",
      "33    PSSA3.SA     2025-10-28              0.906294\n",
      "107   MOTV3.SA     2025-10-28              0.903912\n",
      "70   BPAC11.SA     2025-10-28              0.903755\n",
      "6     EMBR3.SA     2025-10-28              0.897273\n",
      "40    JHSF3.SA     2025-10-28              0.896476\n",
      "76    NEOE3.SA     2025-10-28              0.896137\n",
      "\n",
      "--- PIORES 10 (Menor Probabilidade de ALTA) ---\n",
      "       ticker last_data_date  predicted_proba_ALTA\n",
      "94   VAMO3.SA     2025-10-28              0.002966\n",
      "14   TUPY3.SA     2025-10-28              0.002945\n",
      "36   CSAN3.SA     2025-10-28              0.001386\n",
      "26   AZEV3.SA     2025-10-28              0.001372\n",
      "15   BRKM5.SA     2025-10-28              0.001168\n",
      "105  BRST3.SA     2025-10-28              0.000951\n",
      "101  RAIZ4.SA     2025-10-28              0.000330\n",
      "29   GFSA3.SA     2025-10-28              0.000295\n",
      "71   AZUL4.SA     2025-10-28              0.000290\n",
      "30   NATU3.SA     2025-10-28              0.000258\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Geração de Recomendações (baseado em probabilidade)\n",
    "\n",
    "# --- SESSÃO DE PREVISÃO E RECOMENDAÇÃO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os últimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequências de entrada para a previsão\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequência\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as últimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar à lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previsões (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequências em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previsões para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previsão (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomendações\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequência válida pôde ser criada para previsão.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' não foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0539bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando cálculo de Permutation Feature Importance (usando ROC AUC)...\n",
      "Testando 81 features...\n",
      "Calculando ROC AUC base...\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "ROC AUC Base: 0.5163\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  1/81: close -> Drop: +0.0000 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  2/81: high -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  3/81: low -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  4/81: open -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  5/81: volume -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  6/81: EMA_9 -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  7/81: SMA_21 -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  8/81: SMA_50 -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  9/81: SMA_200 -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  10/81: RSI_14 -> Drop: -0.0006 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  11/81: MACD_12_26_9 -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  12/81: MACDh_12_26_9 -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  13/81: MACDs_12_26_9 -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  14/81: BBL_20_2.0_2.0 -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  15/81: BBM_20_2.0_2.0 -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  16/81: BBU_20_2.0_2.0 -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  17/81: BBB_20_2.0_2.0 -> Drop: +0.0004 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  18/81: BBP_20_2.0_2.0 -> Drop: -0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  19/81: STOCHk_14_3_3 -> Drop: -0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  20/81: STOCHd_14_3_3 -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  21/81: STOCHh_14_3_3 -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  22/81: OBV -> Drop: +0.0016 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  23/81: ATRr_14 -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  24/81: ADX_14 -> Drop: +0.0005 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  25/81: ADXR_14_2 -> Drop: +0.0005 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  26/81: DMP_14 -> Drop: -0.0027 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  27/81: DMN_14 -> Drop: -0.0004 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  28/81: close_sem -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  29/81: high_sem -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  30/81: low_sem -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  31/81: open_sem -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  32/81: volume_sem -> Drop: -0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  33/81: EMA_9_sem -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  34/81: SMA_21_sem -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  35/81: SMA_50_sem -> Drop: -0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  36/81: SMA_200_sem -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  37/81: RSI_14_sem -> Drop: +0.0039 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  38/81: MACD_12_26_9_sem -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  39/81: MACDh_12_26_9_sem -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  40/81: MACDs_12_26_9_sem -> Drop: +0.0000 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  41/81: BBL_20_2.0_2.0_sem -> Drop: -0.0000 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  42/81: BBM_20_2.0_2.0_sem -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  43/81: BBU_20_2.0_2.0_sem -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  44/81: BBB_20_2.0_2.0_sem -> Drop: +0.0003 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "  45/81: BBP_20_2.0_2.0_sem -> Drop: -0.0000 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  46/81: STOCHk_14_3_3_sem -> Drop: +0.0017 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  47/81: STOCHd_14_3_3_sem -> Drop: +0.0015 (7.8s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  48/81: STOCHh_14_3_3_sem -> Drop: +0.0042 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  49/81: OBV_sem -> Drop: +0.0011 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  50/81: ATRr_14_sem -> Drop: +0.0000 (7.9s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  51/81: ADX_14_sem -> Drop: +0.0008 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  52/81: ADXR_14_2_sem -> Drop: +0.0002 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  53/81: DMP_14_sem -> Drop: +0.0037 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  54/81: DMN_14_sem -> Drop: -0.0021 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  55/81: close_men -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  56/81: high_men -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  57/81: low_men -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  58/81: open_men -> Drop: +0.0000 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  59/81: volume_men -> Drop: -0.0001 (8.0s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  60/81: EMA_9_men -> Drop: +0.0000 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  61/81: SMA_21_men -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  62/81: SMA_50_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  63/81: SMA_200_men -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  64/81: RSI_14_men -> Drop: +0.0015 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  65/81: MACD_12_26_9_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  66/81: MACDh_12_26_9_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  67/81: MACDs_12_26_9_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  68/81: BBL_20_2.0_2.0_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  69/81: BBM_20_2.0_2.0_men -> Drop: +0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  70/81: BBU_20_2.0_2.0_men -> Drop: -0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  71/81: BBB_20_2.0_2.0_men -> Drop: +0.0005 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  72/81: BBP_20_2.0_2.0_men -> Drop: -0.0001 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  73/81: STOCHk_14_3_3_men -> Drop: -0.0273 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  74/81: STOCHd_14_3_3_men -> Drop: -0.0188 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  75/81: STOCHh_14_3_3_men -> Drop: +0.0287 (8.1s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  76/81: OBV_men -> Drop: +0.0008 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  77/81: ATRr_14_men -> Drop: -0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  78/81: ADX_14_men -> Drop: +0.0015 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  79/81: ADXR_14_2_men -> Drop: +0.0011 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  80/81: DMP_14_men -> Drop: -0.0000 (8.2s)\n",
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  81/81: DMN_14_men -> Drop: +0.0027 (8.1s)\n",
      "\n",
      "--- TOP 15 Features Mais Importantes (baseado em ROC AUC) ---\n",
      "| feature           |   importance_drop |\n",
      "|:------------------|------------------:|\n",
      "| STOCHh_14_3_3_men |       0.0287128   |\n",
      "| STOCHh_14_3_3_sem |       0.00419206  |\n",
      "| RSI_14_sem        |       0.00392134  |\n",
      "| DMP_14_sem        |       0.00366898  |\n",
      "| DMN_14_men        |       0.0026657   |\n",
      "| STOCHk_14_3_3_sem |       0.00170687  |\n",
      "| OBV               |       0.0016227   |\n",
      "| STOCHd_14_3_3_sem |       0.00153532  |\n",
      "| ADX_14_men        |       0.00152851  |\n",
      "| RSI_14_men        |       0.00150723  |\n",
      "| ADXR_14_2_men     |       0.00113861  |\n",
      "| OBV_sem           |       0.00107439  |\n",
      "| OBV_men           |       0.000775071 |\n",
      "| ADX_14_sem        |       0.000758022 |\n",
      "| ADX_14            |       0.000524256 |\n",
      "\n",
      "--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\n",
      "| feature            |   importance_drop |\n",
      "|:-------------------|------------------:|\n",
      "| DMP_14_men         |      -2.38828e-05 |\n",
      "| SMA_200_men        |      -4.48084e-05 |\n",
      "| BBP_20_2.0_2.0_men |      -7.52706e-05 |\n",
      "| volume_men         |      -8.83569e-05 |\n",
      "| DMN_14             |      -0.000380888 |\n",
      "| RSI_14             |      -0.000589122 |\n",
      "| DMN_14_sem         |      -0.00210128  |\n",
      "| DMP_14             |      -0.00272118  |\n",
      "| STOCHd_14_3_3_men  |      -0.0187746   |\n",
      "| STOCHk_14_3_3_men  |      -0.0272686   |\n",
      "\n",
      "--- Cálculo de Permutation Importance Concluído ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 0. Garanta que temos os dados e o modelo\n",
    "if 'model_transformer' in locals() and 'X_test_seq' in locals() and 'y_test_seq' in locals() and 'features' in locals():\n",
    "    \n",
    "    print(\"Iniciando cálculo de Permutation Feature Importance (usando ROC AUC)...\")\n",
    "    print(f\"Testando {len(features)} features...\")\n",
    "\n",
    "    # 1. Calcular o ROC AUC Base (Baseline)\n",
    "    print(\"Calculando ROC AUC base...\")\n",
    "    # Usamos as probabilidades (saída bruta do modelo)\n",
    "    y_pred_base_proba = model_transformer.predict(X_test_seq).flatten() \n",
    "    baseline_roc_auc = roc_auc_score(y_test_seq, y_pred_base_proba) # <-- MUDANÇA\n",
    "    print(f\"ROC AUC Base: {baseline_roc_auc:.4f}\") # <-- MUDANÇA\n",
    "\n",
    "    importances = []\n",
    "    \n",
    "    # 2. Loop por CADA feature\n",
    "    for i, feature_name in enumerate(features):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Criar uma cópia dos dados de teste\n",
    "        X_test_permuted = np.copy(X_test_seq)\n",
    "        \n",
    "        # 3. Embaralhar (permutar) os valores APENAS da feature 'i'\n",
    "        values_to_shuffle = X_test_permuted[:, :, i].flatten()\n",
    "        np.random.shuffle(values_to_shuffle)\n",
    "        X_test_permuted[:, :, i] = values_to_shuffle.reshape(\n",
    "            (X_test_seq.shape[0], X_test_seq.shape[1])\n",
    "        )\n",
    "        \n",
    "        # 4. Fazer novas previsões com os dados embaralhados\n",
    "        y_pred_permuted_proba = model_transformer.predict(X_test_permuted).flatten()\n",
    "        \n",
    "        # 5. Calcular o novo ROC AUC\n",
    "        permuted_roc_auc = roc_auc_score(y_test_seq, y_pred_permuted_proba) # <-- MUDANÇA\n",
    "        \n",
    "        # 6. Salvar a QUEDA de importância (na métrica ROC AUC)\n",
    "        importance_drop = baseline_roc_auc - permuted_roc_auc # <-- MUDANÇA\n",
    "        importances.append({\n",
    "            'feature': feature_name,\n",
    "            'importance_drop': importance_drop\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        # <-- MUDANÇA no print para mostrar melhor o drop do AUC\n",
    "        print(f\"  {i+1}/{len(features)}: {feature_name} -> Drop: {importance_drop:+.4f} ({(end_time-start_time):.1f}s)\")\n",
    "\n",
    "\n",
    "    # Converte a lista de resultados em um DataFrame e ordena pela importância\n",
    "    df_importances = pd.DataFrame(importances).sort_values(by='importance_drop', ascending=False)\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(\"\\n--- TOP 15 Features Mais Importantes (baseado em ROC AUC) ---\")\n",
    "    print(df_importances.head(15).to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\")\n",
    "    print(df_importances.tail(10).to_markdown(index=False))\n",
    "    \n",
    "    print(\"\\n--- Cálculo de Permutation Importance Concluído ---\")\n",
    "    # df_importances # O DataFrame será a saída da célula\n",
    "    \n",
    "else:\n",
    "    print(\"ERRO: Variáveis necessárias (modelo, X_test_seq, etc.) não encontradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f1a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features originais: 81\n",
      "Features restantes (v2): 61\n",
      "\n",
      "Features selecionadas (as únicas que ajudam o modelo):\n",
      "['STOCHh_14_3_3_men', 'STOCHh_14_3_3_sem', 'RSI_14_sem', 'DMP_14_sem', 'DMN_14_men', 'STOCHk_14_3_3_sem', 'OBV', 'STOCHd_14_3_3_sem', 'ADX_14_men', 'RSI_14_men', 'ADXR_14_2_men', 'OBV_sem', 'OBV_men', 'ADX_14_sem', 'ADX_14', 'ADXR_14_2', 'BBB_20_2.0_2.0_men', 'BBB_20_2.0_2.0', 'BBB_20_2.0_2.0_sem', 'ADXR_14_2_sem', 'ATRr_14_sem', 'MACDs_12_26_9_men', 'ATRr_14', 'SMA_50_men', 'SMA_200_sem', 'close_sem', 'EMA_9_sem', 'MACDh_12_26_9_men', 'SMA_21_sem', 'SMA_50', 'close', 'open', 'BBL_20_2.0_2.0', 'MACD_12_26_9_men', 'SMA_21_men', 'EMA_9_men', 'BBL_20_2.0_2.0_men', 'close_men', 'EMA_9', 'BBU_20_2.0_2.0_sem', 'low_men', 'high_men', 'SMA_200', 'MACD_12_26_9_sem', 'MACDs_12_26_9', 'MACD_12_26_9', 'BBU_20_2.0_2.0', 'BBM_20_2.0_2.0_men', 'low_sem', 'SMA_21', 'low', 'open_men', 'BBM_20_2.0_2.0_sem', 'high_sem', 'BBM_20_2.0_2.0', 'open_sem', 'high', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'MACDh_12_26_9', 'STOCHd_14_3_3']\n"
     ]
    }
   ],
   "source": [
    "# Adicione esta nova célula\n",
    "# Filtra o DataFrame para pegar apenas features com importância positiva\n",
    "df_features_v2 = df_importances[df_importances['importance_drop'] > 0]\n",
    "\n",
    "# Cria a nova lista de features\n",
    "features_v2 = df_features_v2['feature'].tolist()\n",
    "\n",
    "print(f\"Features originais: {len(features)}\")\n",
    "print(f\"Features restantes (v2): {len(features_v2)}\")\n",
    "print(\"\\nFeatures selecionadas (as únicas que ajudam o modelo):\")\n",
    "print(features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e183aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- USANDO FEATURE SET V2 (OTIMIZADO) ---\n",
      "Usando 61 features selecionadas:\n",
      "['STOCHh_14_3_3_men', 'STOCHh_14_3_3_sem', 'RSI_14_sem', 'DMP_14_sem', 'DMN_14_men', 'STOCHk_14_3_3_sem', 'OBV', 'STOCHd_14_3_3_sem', 'ADX_14_men', 'RSI_14_men', 'ADXR_14_2_men', 'OBV_sem', 'OBV_men', 'ADX_14_sem', 'ADX_14', 'ADXR_14_2', 'BBB_20_2.0_2.0_men', 'BBB_20_2.0_2.0', 'BBB_20_2.0_2.0_sem', 'ADXR_14_2_sem', 'ATRr_14_sem', 'MACDs_12_26_9_men', 'ATRr_14', 'SMA_50_men', 'SMA_200_sem', 'close_sem', 'EMA_9_sem', 'MACDh_12_26_9_men', 'SMA_21_sem', 'SMA_50', 'close', 'open', 'BBL_20_2.0_2.0', 'MACD_12_26_9_men', 'SMA_21_men', 'EMA_9_men', 'BBL_20_2.0_2.0_men', 'close_men', 'EMA_9', 'BBU_20_2.0_2.0_sem', 'low_men', 'high_men', 'SMA_200', 'MACD_12_26_9_sem', 'MACDs_12_26_9', 'MACD_12_26_9', 'BBU_20_2.0_2.0', 'BBM_20_2.0_2.0_men', 'low_sem', 'SMA_21', 'low', 'open_men', 'BBM_20_2.0_2.0_sem', 'high_sem', 'BBM_20_2.0_2.0', 'open_sem', 'high', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'MACDh_12_26_9', 'STOCHd_14_3_3']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Definição das Features (X) ---\n",
    "# V2 - Usando a seleção de features da Célula [11] para combater overfitting\n",
    "\n",
    "# A variável 'features' agora será esta lista otimizada\n",
    "features = features_v2\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final está vazio. Rode as células [2] e [3] primeiro.\")\n",
    "else:\n",
    "    # Garante que as colunas 'y' não estão em 'features'\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    features = [col for col in features if col not in colunas_a_excluir]\n",
    "\n",
    "    print(f\"--- USANDO FEATURE SET V2 (OTIMIZADO) ---\")\n",
    "    print(f\"Usando {len(features)} features selecionadas:\")\n",
    "    print(features)\n",
    "    \n",
    "    # A verificação de Leakage continua importante\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO: {leaky_cols} !!!\")\n",
    "    else:\n",
    "        print(\">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    \n",
    "    # Definir X e y para as células seguintes\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e99243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (253233, 61)\n",
      "Shape Validação 2D (X_validation): (63309, 61)\n",
      "Shape Teste 2D (X_test): (79136, 61)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2019-12-12 00:00:00\n",
      "Datas de Validação: 2019-12-12 00:00:00 a 2022-09-05 00:00:00\n",
      "Datas de Teste: 2022-09-05 00:00:00 a 2025-09-16 00:00:00\n",
      "\n",
      "X_train_scaled shape: (253233, 61)\n",
      "X_val_scaled shape: (63309, 61)\n",
      "X_test_scaled shape: (79136, 61)\n"
     ]
    }
   ],
   "source": [
    "# A Partir daqui refaz o modelo treinando as features mais relevantes\n",
    "# <-- MUDANÇA: Solução 2 - Split Treino/Validação/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+validação\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Validação (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Validação 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Validação: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normalização (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os três sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a função de sequência)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593ec072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequências de TREINO...\n",
      "Criando sequências de VALIDAÇÃO...\n",
      "Criando sequências de TESTE...\n",
      "\n",
      "Formato das Sequências de Treino (X): (250884, 30, 61)\n",
      "Formato dos Alvos de Treino (y): (250884,)\n",
      "\n",
      "Formato das Sequências de Validação (X): (60209, 30, 61)\n",
      "Formato dos Alvos de Validação (y): (60209,)\n",
      "\n",
      "Formato das Sequências de Teste (X): (75896, 30, 61)\n",
      "Formato dos Alvos de Teste (y): (75896,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequências de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequências não cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os índices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker específico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O índice original é mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele será ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequência de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # Índice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDANÇA: Solução 2 - Criar Sequências para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequências de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de VALIDAÇÃO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta é a variável que a Célula [9] (Avaliação) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequências de Validação (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Validação (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste não encontrados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77eb1763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de Classe Calculados: {np.int64(0): np.float64(0.7946659909410535), np.int64(1): np.float64(1.348418235173978)}\n",
      "Reloading Tuner from keras_tuner_dir\\stock_TRANSFORMER_tuning_weighted_rocauc_featuresv2\\tuner0.json\n",
      "--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\n",
      "\n",
      "Search: Running Trial #60\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |1                 |num_blocks\n",
      "4                 |4                 |num_heads\n",
      "32                |64                |head_size\n",
      "68                |34                |ff_dim\n",
      "0.1               |0.1               |dropout\n",
      "8.2017e-05        |0.00043115        |learning_rate\n",
      "32                |64                |dense_units\n",
      "6                 |6                 |tuner/epochs\n",
      "0                 |2                 |tuner/initial_epoch\n",
      "2                 |3                 |tuner/bracket\n",
      "0                 |1                 |tuner/round\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Iniciar a busca\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- BUSCA CONCLUÍDA ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# 1. Pegar os melhores hiperparâmetros\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[39m, in \u001b[36mBaseTuner.search\u001b[39m\u001b[34m(self, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_begin(trial)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_end(trial)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.on_search_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[39m, in \u001b[36mBaseTuner._try_run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m         trial.status = trial_module.TrialStatus.COMPLETED\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[39m, in \u001b[36mBaseTuner._run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.oracle.objective.name\n\u001b[32m    242\u001b[39m     ):\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[32m    245\u001b[39m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[32m    246\u001b[39m         warnings.warn(\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe use case of calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    255\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\tuners\\hyperband.py:427\u001b[39m, in \u001b[36mHyperband.run_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/epochs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    426\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33minitial_epoch\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/initial_epoch\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[39m, in \u001b[36mTuner.run_trial\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     callbacks.append(model_checkpoint)\n\u001b[32m    313\u001b[39m     copied_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m] = callbacks\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     obj_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     histories.append(obj_value)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[39m, in \u001b[36mTuner._build_and_fit_model\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hp = trial.hyperparameters\n\u001b[32m    232\u001b[39m model = \u001b[38;5;28mself\u001b[39m._try_build(hp)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.config.multi_backend():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[39m, in \u001b[36mHyperModel.fit\u001b[39m\u001b[34m(self, hp, model, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, *args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:338\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    331\u001b[39m     (\n\u001b[32m    332\u001b[39m         val_x,\n\u001b[32m    333\u001b[39m         val_y,\n\u001b[32m    334\u001b[39m         val_sample_weight,\n\u001b[32m    335\u001b[39m     ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m epoch_iterator = \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_symbolic_build(iterator=epoch_iterator)\n\u001b[32m    351\u001b[39m epoch_iterator.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:728\u001b[39m, in \u001b[36mTFEpochIterator.__init__\u001b[39m\u001b[34m(self, distribute_strategy, *args, **kwargs)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m    727\u001b[39m \u001b[38;5;28mself\u001b[39m._distribute_strategy = distribute_strategy\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf.distribute.DistributedDataset):\n\u001b[32m    730\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._distribute_strategy.experimental_distribute_dataset(\n\u001b[32m    731\u001b[39m         dataset\n\u001b[32m    732\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    233\u001b[39m     indices_dataset = indices_dataset.map(tf.random.shuffle)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m dataset = \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m options = tf.data.Options()\n\u001b[32m    238\u001b[39m options.experimental_distribute.auto_shard_policy = (\n\u001b[32m    239\u001b[39m     tf.data.experimental.AutoShardPolicy.DATA\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:191\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[39m\u001b[34m(indices_dataset, inputs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice_inputs\u001b[39m(indices_dataset, inputs):\n\u001b[32m    177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Slice inputs into a Dataset of batches.\u001b[39;00m\n\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    Given a Dataset of batch indices and the unsliced inputs,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33;03m        A Dataset of input batches matching the batch indices.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     inputs = \u001b[43marray_slicing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_sliceable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorflow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     inputs = tree.lists_to_tuples(inputs)\n\u001b[32m    196\u001b[39m     dataset = tf.data.Dataset.zip(\n\u001b[32m    197\u001b[39m         (indices_dataset, tf.data.Dataset.from_tensors(inputs).repeat())\n\u001b[32m    198\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:458\u001b[39m, in \u001b[36mconvert_to_sliceable\u001b[39m\u001b[34m(arrays, target_backend)\u001b[39m\n\u001b[32m    454\u001b[39m         sliceable_class = NumpySliceable\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sliceable_class(x)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_single_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:192\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.tree.map_structure\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_structure\u001b[39m(func, *structures):\n\u001b[32m    164\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Maps `func` through given structures.\u001b[39;00m\n\u001b[32m    165\u001b[39m \n\u001b[32m    166\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m \u001b[33;03m            `assert_same_structure`.\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:111\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args)\n\u001b[32m    109\u001b[39m map_func = func_with_check \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(structures) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m func\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnone_is_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkeras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optree\\ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:435\u001b[39m, in \u001b[36mconvert_to_sliceable.<locals>.convert_single_array\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    432\u001b[39m         cast_dtype = backend.floatx()\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cast_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     x = \u001b[43msliceable_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Step 3. Apply target backend specific logic and optimizations.\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:63\u001b[39m, in \u001b[36mSliceable.cast\u001b[39m\u001b[34m(cls, x, dtype)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcast\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x, dtype):\n\u001b[32m     55\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Cast a tensor to a different dtype.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03m    Only called on a full array as provided by the user.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m \u001b[33;03m    Returns: the cast tensor.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MODEL_FILE = 'transformers_stock_model_30d_ts30_CLASS_weighted_featurev2'\n",
    "\n",
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um único bloco Transformer Encoder.\n",
    "    d_model = dimensão da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta atenção\" a diferentes partes da sequência de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimensão original\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Função Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Função construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimensões dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimensão do modelo é o número de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro não sabe a *ordem* dos dias (é permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprendível\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posições (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posições (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posição aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperparâmetros para Tunar ---\n",
    "    \n",
    "    # 1. Número de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Parâmetros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabeças\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimensão da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regularização)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=5e-4, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que você tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constrói a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabeça de Classificação (Classification Head) ---\n",
    "    \n",
    "    # A saída 'x' ainda é uma sequência (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um único vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a média dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classificação\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classificação (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[AUC(name='roc_auc', curve='ROC'), 'accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configuração do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes únicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores às classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicionário de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq não encontrado. Pulando cálculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados não estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective=kt.Objective(\"val_roc_auc\", direction=\"max\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted_rocauc_featuresv2'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLUÍDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperparâmetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperparâmetros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Pegar o melhor modelo\n",
    "    model_transformer = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    print(\"Melhor modelo (Transformer) carregado na variável 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # Salvar o melhor modelo\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' não foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cee21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step\n",
      "\n",
      "Acurácia (Accuracy) no Teste: 37.10%\n",
      "ROC AUC no Teste: 0.5413\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[ 3904 46352]\n",
      " [ 1386 24254]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BAIXA (0)       0.74      0.08      0.14     50256\n",
      "    ALTA (1)       0.34      0.95      0.50     25640\n",
      "\n",
      "    accuracy                           0.37     75896\n",
      "   macro avg       0.54      0.51      0.32     75896\n",
      "weighted avg       0.60      0.37      0.26     75896\n",
      "\n",
      "\n",
      "Amostra dos Resultados (Transformer):\n",
      "             date    ticker  y_real  y_pred_proba  y_pred_class\n",
      "331912 2022-10-19  BBAS3.SA       0      0.509131             1\n",
      "331940 2022-10-20  BBAS3.SA       0      0.491063             0\n",
      "332045 2022-10-21  BBAS3.SA       0      0.493700             0\n",
      "332183 2022-10-24  BBAS3.SA       0      0.495979             0\n",
      "332297 2022-10-25  BBAS3.SA       0      0.537875             1\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Avaliação de Classificação\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previsões no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # 3. Buscar as informações originais (data, ticker) usando os índices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avaliação de Métricas de Classificação\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcurácia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # --- CÁLCULO DE ROC AUC ADICIONADO ---\n",
    "    # (Usa as probabilidades, não as classes)\n",
    "    roc_auc = roc_auc_score(df_results['y_real'], df_results['y_pred_proba'])\n",
    "    print(f\"ROC AUC no Teste: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (Transformer):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2da607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\n",
      "Usando modelo treinado para prever a PROBABILIDADE de alta em 30 dias.\n",
      "Usando os últimos 30 dias de dados como entrada.\n",
      "==================================================\n",
      "\n",
      "Gerando previsões para 108 tickers...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\n",
      "--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\n",
      "       ticker last_data_date  predicted_proba_ALTA\n",
      "75   HAPV3.SA     2025-10-28              0.950293\n",
      "44   MBRF3.SA     2025-10-28              0.949309\n",
      "15   BRKM5.SA     2025-10-28              0.944970\n",
      "69   RAIL3.SA     2025-10-28              0.920667\n",
      "89   ENJU3.SA     2025-10-28              0.919847\n",
      "33   DASA3.SA     2025-10-28              0.918551\n",
      "47   DXCO3.SA     2025-10-28              0.916227\n",
      "40   EVEN3.SA     2025-10-28              0.902683\n",
      "68  KLBN11.SA     2025-10-28              0.902186\n",
      "1    LIGT3.SA     2025-10-28              0.901526\n",
      "\n",
      "--- PIORES 10 (Menor Probabilidade de ALTA) ---\n",
      "       ticker last_data_date  predicted_proba_ALTA\n",
      "103  AURE3.SA     2025-10-28              0.687530\n",
      "107  MOTV3.SA     2025-10-28              0.687050\n",
      "87   MELK3.SA     2025-10-28              0.646568\n",
      "105  BRST3.SA     2025-10-28              0.630367\n",
      "45   BEEF3.SA     2025-10-28              0.612183\n",
      "99   CBAV3.SA     2025-10-28              0.600134\n",
      "24   BIOM3.SA     2025-10-28              0.599834\n",
      "19   BRFS3.SA     2025-09-22              0.531418\n",
      "31   NATU3.SA     2025-10-28              0.499783\n",
      "51   BPAN4.SA     2025-10-28              0.421799\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Geração de Recomendações (baseado em probabilidade)\n",
    "\n",
    "# --- SESSÃO DE PREVISÃO E RECOMENDAÇÃO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os últimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequências de entrada para a previsão\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequência\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as últimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar à lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previsões (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequências em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previsões para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previsão (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomendações\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequência válida pôde ser criada para previsão.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' não foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f293a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
