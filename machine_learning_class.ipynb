{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ade6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75db9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diário: (411144, 29)\n",
      "Semanal: (85971, 29)\n",
      "Mensal: (19802, 29)\n",
      "\n",
      "Tipo da coluna 'datetime' (Diário): datetime64[ns]\n",
      "\n",
      "Amostra Dados Diários:\n",
      "    datetime     close      high       low      open  volume    ticker  EMA_9  \\\n",
      "0 2000-01-05  0.248809  0.248809  0.248809  0.248809     985  ABEV3.SA    NaN   \n",
      "1 2000-01-06  0.236196  0.236196  0.236196  0.236196     227  ABEV3.SA    NaN   \n",
      "2 2000-01-07  0.236196  0.236196  0.236196  0.236196     151  ABEV3.SA    NaN   \n",
      "3 2000-01-10  0.236196  0.236196  0.236196  0.236196    1516  ABEV3.SA    NaN   \n",
      "4 2000-01-11  0.236196  0.236196  0.236196  0.236196    3791  ABEV3.SA    NaN   \n",
      "\n",
      "   SMA_21  SMA_50  ...  BBP_20_2.0_2.0  STOCHk_14_3_3  STOCHd_14_3_3  \\\n",
      "0     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "1     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "2     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "3     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "4     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "\n",
      "   STOCHh_14_3_3    OBV  ATRr_14  ADX_14  ADXR_14_2  DMP_14  DMN_14  \n",
      "0            NaN    NaN      NaN     NaN        NaN     NaN     NaN  \n",
      "1            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "2            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "3            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "4            NaN -227.0      NaN     NaN        NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Conectar ao banco de dados SQLite\n",
    "DB_FILE = \"dados_acoes.db\"\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "\n",
    "# Carregar as tabelas\n",
    "# O usuário confirmou que as colunas 'date' já estão em formato datetime\n",
    "try:\n",
    "    df_diario = pd.read_sql_query(\"SELECT * FROM diario_com_indicadores\", conn)\n",
    "    df_semanal = pd.read_sql_query(\"SELECT * FROM semanal_com_indicadores\", conn)\n",
    "    df_mensal = pd.read_sql_query(\"SELECT * FROM mensal_com_indicadores\", conn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Verificação extra: garantir que o pandas as reconheceu como datetime\n",
    "    # Se elas forem strings, o merge_asof falhará.\n",
    "    df_diario['datetime'] = pd.to_datetime(df_diario['datetime'])\n",
    "    df_semanal['datetime'] = pd.to_datetime(df_semanal['datetime'])\n",
    "    df_mensal['datetime'] = pd.to_datetime(df_mensal['datetime'])\n",
    "\n",
    "    print(f\"Diário: {df_diario.shape}\")\n",
    "    print(f\"Semanal: {df_semanal.shape}\")\n",
    "    print(f\"Mensal: {df_mensal.shape}\")\n",
    "    \n",
    "    print(\"\\nTipo da coluna 'datetime' (Diário):\", df_diario['datetime'].dtype)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    print(\"Verifique se o nome do arquivo 'dados_acoes.db' e os nomes das tabelas estão corretos.\")\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# Visualizar os dados\n",
    "print(\"\\nAmostra Dados Diários:\")\n",
    "print(df_diario.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amostra de Dados Unificados:\n",
      "    datetime       close        high         low        open    volume  \\\n",
      "0 2000-01-03    0.444428    0.463940    0.444428    0.455268   1029600   \n",
      "1 2000-01-03   53.865044   53.865044   53.865044   53.865044       145   \n",
      "2 2000-01-03    0.327688    0.345801    0.306281    0.307928  13152318   \n",
      "3 2000-01-03  129.651276  131.126621  128.161031  129.651276    150000   \n",
      "4 2000-01-03  214.932358  217.591315  212.716560  213.159720   1210000   \n",
      "\n",
      "     ticker      EMA_9     SMA_21     SMA_50  ...  BBP_20_2.0_2.0_men  \\\n",
      "0  BBAS3.SA  10.141362  12.038306  12.614476  ...           -0.404727   \n",
      "1  LIGT3.SA  24.982800  19.414526  18.789501  ...            1.552200   \n",
      "2  ITSA4.SA  38.178534  45.309890  46.331954  ...           -0.385870   \n",
      "3  GOAU4.SA  30.972806  12.267680   9.200727  ...            1.561928   \n",
      "4  GGBR4.SA  54.807740  24.783922  19.350447  ...            1.561622   \n",
      "\n",
      "   STOCHk_14_3_3_men  STOCHd_14_3_3_men  STOCHh_14_3_3_men       OBV_men  \\\n",
      "0          53.698555          65.250575         -11.552019 -9.521600e+09   \n",
      "1          62.569316          38.593369          23.975947 -4.125575e+11   \n",
      "2          51.173148          63.283140         -12.109992 -4.771428e+11   \n",
      "3          71.016681          62.006443           9.010239 -4.791943e+11   \n",
      "4          90.755441          87.918217           2.837224 -4.812348e+11   \n",
      "\n",
      "   ATRr_14_men  ADX_14_men  ADXR_14_2_men  DMP_14_men  DMN_14_men  \n",
      "0     3.312594   20.812228      18.824192   11.536864   30.955799  \n",
      "1     6.124019   22.479420      20.094992   60.270481   24.294587  \n",
      "2    15.464539   17.066745      15.226792   12.460076   29.143012  \n",
      "3    11.316480   14.718549      10.508759   87.158421    5.986637  \n",
      "4    19.765694   22.056788      18.280244   89.403677    6.186510  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "Shape após merge: (411144, 83)\n"
     ]
    }
   ],
   "source": [
    "# Garantir que tudo está ordenado por data para o merge_asof funcionar\n",
    "df_diario = df_diario.sort_values(by='datetime')\n",
    "df_semanal = df_semanal.sort_values(by='datetime')\n",
    "df_mensal = df_mensal.sort_values(by='datetime')\n",
    "\n",
    "# Renomear colunas de indicadores para evitar conflitos (ex: 'RSI' diário, 'RSI' semanal)\n",
    "df_semanal = df_semanal.add_suffix('_sem')\n",
    "df_mensal = df_mensal.add_suffix('_men')\n",
    "\n",
    "# Renomear colunas de junção\n",
    "df_semanal = df_semanal.rename(columns={'datetime_sem': 'datetime', 'ticker_sem': 'ticker'})\n",
    "df_mensal = df_mensal.rename(columns={'datetime_men': 'datetime', 'ticker_men': 'ticker'})\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Junção (Merge)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Juntar Diário com Semanal\n",
    "# Para cada 'ticker', vamos juntar a data diária com a data semanal mais próxima (anterior ou igual)\n",
    "df_merged = pd.merge_asof(\n",
    "    df_diario,\n",
    "    df_semanal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward' # 'backward' pega o último dado semanal disponível para aquele dia\n",
    ")\n",
    "\n",
    "# 2. Juntar o resultado com o Mensal\n",
    "df_merged = pd.merge_asof(\n",
    "    df_merged,\n",
    "    df_mensal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "print(\"\\nAmostra de Dados Unificados:\")\n",
    "print(df_merged.head())\n",
    "\n",
    "print(f\"\\nShape após merge: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981b6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definição do Alvo (y) ---\n",
    "PERIOD_HORIZON = 15\n",
    "TIME_STEPS = 30 # Hiperparâmetro: quantos dias o LSTM vai \"olhar para trás\"\n",
    "\n",
    "# <-- MUDANÇA: Threshold para classificação binária (0 = retorno > 0%)\n",
    "CLASSIFICATION_THRESHOLD = 0.05\n",
    "\n",
    "# THRESHOLD\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "# <-- MUDANÇA: Novo nome de arquivo para o modelo de classificação\n",
    "MODEL_FILE = \"transformers_stock_model_30d_ts30_CLASS_weighted.keras\" # Nome do arquivo para salvar/carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b2d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape final após limpeza e 'y_target': (397298, 86)\n",
      "Distribuição do Alvo (y_target):\n",
      "y_target\n",
      "0    0.711564\n",
      "1    0.288436\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Usando 81 features:\n",
      "['close', 'high', 'low', 'open', 'volume', 'EMA_9', 'SMA_21', 'SMA_50', 'SMA_200', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_20_2.0_2.0', 'BBM_20_2.0_2.0', 'BBU_20_2.0_2.0', 'BBB_20_2.0_2.0', 'BBP_20_2.0_2.0', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'STOCHh_14_3_3', 'OBV', 'ATRr_14', 'ADX_14', 'ADXR_14_2', 'DMP_14', 'DMN_14', 'close_sem', 'high_sem', 'low_sem', 'open_sem', 'volume_sem', 'EMA_9_sem', 'SMA_21_sem', 'SMA_50_sem', 'SMA_200_sem', 'RSI_14_sem', 'MACD_12_26_9_sem', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'BBL_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'BBU_20_2.0_2.0_sem', 'BBB_20_2.0_2.0_sem', 'BBP_20_2.0_2.0_sem', 'STOCHk_14_3_3_sem', 'STOCHd_14_3_3_sem', 'STOCHh_14_3_3_sem', 'OBV_sem', 'ATRr_14_sem', 'ADX_14_sem', 'ADXR_14_2_sem', 'DMP_14_sem', 'DMN_14_sem', 'close_men', 'high_men', 'low_men', 'open_men', 'volume_men', 'EMA_9_men', 'SMA_21_men', 'SMA_50_men', 'SMA_200_men', 'RSI_14_men', 'MACD_12_26_9_men', 'MACDh_12_26_9_men', 'MACDs_12_26_9_men', 'BBL_20_2.0_2.0_men', 'BBM_20_2.0_2.0_men', 'BBU_20_2.0_2.0_men', 'BBB_20_2.0_2.0_men', 'BBP_20_2.0_2.0_men', 'STOCHk_14_3_3_men', 'STOCHd_14_3_3_men', 'STOCHh_14_3_3_men', 'OBV_men', 'ATRr_14_men', 'ADX_14_men', 'ADXR_14_2_men', 'DMP_14_men', 'DMN_14_men']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por ticker para calcular o shift corretamente\n",
    "df_merged['close_future'] = df_merged.groupby('ticker')['close'].shift(-PERIOD_HORIZON)\n",
    "\n",
    "\n",
    "# 1. Calcular o retorno futuro (para referência e para criar o alvo)\n",
    "df_merged['y_return'] = (df_merged['close_future'] / df_merged['close']) - 1\n",
    "\n",
    "# 2. Criar o alvo de CLASSIFICAÇÃO (1 se o retorno > threshold, 0 caso contrário)\n",
    "df_merged['y_target'] = (df_merged['y_return'] > CLASSIFICATION_THRESHOLD).astype(int)\n",
    "\n",
    "# --- Limpeza ---\n",
    "# Remover dados onde não pudemos calcular o alvo (os últimos N dias de cada ticker)\n",
    "# Também remover quaisquer NaNs gerados pelos merges ou cálculos de indicadores\n",
    "df_final = df_merged.dropna()\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final está vazio após o dropna().\")\n",
    "    print(\"Verifique seus dados de entrada, a lógica de merge e o cálculo do 'y_target'.\")\n",
    "else:\n",
    "    print(f\"\\nShape final após limpeza e 'y_target': {df_final.shape}\")\n",
    "    print(f\"Distribuição do Alvo (y_target):\\n{df_final['y_target'].value_counts(normalize=True)}\")\n",
    "\n",
    "    # --- Definição das Features (X) ---\n",
    "    features_diarias = [col for col in df_final.columns if col not in ['datetime', 'ticker'] and not col.endswith('_sem') and not col.endswith('_men')]\n",
    "    features_semanais = [col for col in df_final.columns if col.endswith('_sem') and col not in ['date_sem', 'ticker_sem']]\n",
    "    features_mensais = [col for col in df_final.columns if col.endswith('_men') and col not in ['date_men', 'ticker_men']]\n",
    "    all_features = features_diarias + features_semanais + features_mensais\n",
    "    \n",
    "    # Defina as colunas que NUNCA devem ser features\n",
    "    # <-- MUDANÇA: Adicionado 'y_return' à exclusão\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    \n",
    "    # Filtra para garantir que são numéricas E não são as colunas de exclusão\n",
    "    features_numericas = [col for col in all_features if pd.api.types.is_numeric_dtype(df_final[col])]\n",
    "    features = [col for col in features_numericas if col not in colunas_a_excluir]\n",
    "    print(f\"\\nUsando {len(features)} features:\")\n",
    "    print(features)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO !!!\")\n",
    "        print(f\"As seguintes colunas-alvo ESTÃO na sua lista 'features' (X): {leaky_cols}\")\n",
    "        print(f\"Corrija sua lista 'colunas_a_excluir' na Célula [5].\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    else:\n",
    "        print(\">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7465383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (254270, 81)\n",
      "Shape Validação 2D (X_validation): (63568, 81)\n",
      "Shape Teste 2D (X_test): (79460, 81)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2020-01-06 00:00:00\n",
      "Datas de Validação: 2020-01-06 00:00:00 a 2022-09-23 00:00:00\n",
      "Datas de Teste: 2022-09-23 00:00:00 a 2025-10-07 00:00:00\n",
      "\n",
      "X_train_scaled shape: (254270, 81)\n",
      "X_val_scaled shape: (63568, 81)\n",
      "X_test_scaled shape: (79460, 81)\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 2 - Split Treino/Validação/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+validação\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Validação (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Validação 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Validação: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normalização (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os três sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a função de sequência)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5639e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequências de TREINO...\n",
      "Criando sequências de VALIDAÇÃO...\n",
      "Criando sequências de TESTE...\n",
      "\n",
      "Formato das Sequências de Treino (X): (251907, 30, 81)\n",
      "Formato dos Alvos de Treino (y): (251907,)\n",
      "\n",
      "Formato das Sequências de Validação (X): (60458, 30, 81)\n",
      "Formato dos Alvos de Validação (y): (60458,)\n",
      "\n",
      "Formato das Sequências de Teste (X): (76220, 30, 81)\n",
      "Formato dos Alvos de Teste (y): (76220,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequências de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequências não cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os índices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker específico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O índice original é mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele será ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequência de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # Índice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDANÇA: Solução 2 - Criar Sequências para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequências de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de VALIDAÇÃO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta é a variável que a Célula [9] (Avaliação) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequências de Validação (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Validação (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste não encontrados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c230f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de Classe Calculados: {np.int64(0): np.float64(0.704847898106281), np.int64(1): np.float64(1.7204176967941975)}\n",
      "Reloading Tuner from keras_tuner_dir\\stock_TRANSFORMER_tuning_weighted_rocauc\\tuner0.json\n",
      "--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\n",
      "--- BUSCA CONCLUÍDA ---\n",
      "\n",
      "    Melhores hiperparâmetros encontrados:\n",
      "    - num_blocks: 1\n",
      "    - num_heads: 4\n",
      "    - head_size: 64\n",
      "    - ff_dim: 324\n",
      "    - dropout: 0.20\n",
      "    - dense_units: 32\n",
      "    - learning_rate: 0.00202\n",
      "    \n",
      "--- TREINANDO O MODELO FINAL COM OS MELHORES HPs ---\n",
      "Buscando a melhor época do 'best_trial'...\n",
      "Melhor época encontrada (baseada no val_loss): 1\n",
      "Treinando o modelo final por 1 epochs.\n",
      "\u001b[1m 276/3937\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 14ms/step - accuracy: 0.4950 - loss: 0.7018 - roc_auc: 0.5372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTreinando o modelo final por \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# 3. Treinar o modelo final. \u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m#    Desta vez, usamos os mesmos dados de treino E validação.\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m#    O EarlyStopping é usado para garantir que ele pare no ponto certo.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m history = \u001b[43mmodel_transformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Treina pelo número ideal de épocas\u001b[39;49;00m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use o mesmo EarlyStopping\u001b[39;49;00m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModelo final (Transformer) treinado e carregado na variável \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_transformer\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    234\u001b[39m model_transformer.summary()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um único bloco Transformer Encoder.\n",
    "    d_model = dimensão da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta atenção\" a diferentes partes da sequência de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimensão original\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Função Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Função construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimensões dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimensão do modelo é o número de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro não sabe a *ordem* dos dias (é permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprendível\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posições (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posições (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posição aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperparâmetros para Tunar ---\n",
    "    \n",
    "    # 1. Número de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Parâmetros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabeças\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimensão da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regularização)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=5e-4, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que você tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constrói a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabeça de Classificação (Classification Head) ---\n",
    "    \n",
    "    # A saída 'x' ainda é uma sequência (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um único vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a média dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classificação\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classificação (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[AUC(name='roc_auc', curve='ROC'), 'accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configuração do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes únicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores às classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicionário de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq não encontrado. Pulando cálculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados não estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective=kt.Objective(\"val_roc_auc\", direction=\"max\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted_rocauc'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLUÍDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperparâmetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperparâmetros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # SOLUÇÃO: RE-TREINAR O MELHOR MODELO\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"--- TREINANDO O MODELO FINAL COM OS MELHORES HPs ---\")\n",
    "\n",
    "    # 1. Construir o modelo final usando os melhores HPs\n",
    "    #    (best_hps já foi pego na etapa anterior)\n",
    "    model_transformer = build_model(best_hps)\n",
    "\n",
    "    # 2. (Opcional, mas recomendado) Recupere o número ideal de epochs\n",
    "    #    se o EarlyStopping parou o melhor \"trial\" mais cedo.\n",
    "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "    print(\"Buscando a melhor época do 'best_trial'...\")\n",
    "\n",
    "    # 1. Pegar o histórico da métrica que seu EarlyStopping monitora (val_loss)\n",
    "    #    Isso retorna uma lista de objetos: [MetricHistory(epoch=0, value=0.5), MetricHistory(epoch=1, value=0.45), ...]\n",
    "    val_loss_history_objects = best_trial.metrics.get_history(\"val_loss\")\n",
    "\n",
    "    # 2. Extrair APENAS os valores numéricos para o numpy\n",
    "    #    Isso cria uma lista como: [0.5, 0.45, 0.48]\n",
    "    val_loss_values = [m.value for m in val_loss_history_objects]\n",
    "\n",
    "    # 3. Encontrar o índice (época) que teve o *menor* valor de loss\n",
    "    #    Usamos np.argmin() porque \"loss\" deve ser minimizada.\n",
    "    best_epoch_index = np.argmin(val_loss_values)\n",
    "\n",
    "    # 4. Converter o índice (base-0) para o número de épocas (base-1)\n",
    "    #    Se o índice 0 for o melhor, treinamos por 1 época.\n",
    "    #    Se o índice 1 for o melhor, treinamos por 2 épocas.\n",
    "    best_epochs = best_epoch_index + 8\n",
    "\n",
    "    print(f\"Melhor época encontrada (baseada no val_loss): {best_epochs}\")\n",
    "\n",
    "    print(f\"Treinando o modelo final por {best_epochs} epochs.\")\n",
    "\n",
    "    # 3. Treinar o modelo final. \n",
    "    #    Desta vez, usamos os mesmos dados de treino E validação.\n",
    "    #    O EarlyStopping é usado para garantir que ele pare no ponto certo.\n",
    "    history = model_transformer.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_epochs, # Treina pelo número ideal de épocas\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping], # Use o mesmo EarlyStopping\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Modelo final (Transformer) treinado e carregado na variável 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # 4. Salvar o modelo final treinado\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' não foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2de976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\n",
      "Acurácia (Accuracy) no Teste: 73.88%\n",
      "ROC AUC no Teste: 0.7301\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[47344  8050]\n",
      " [11861  8965]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BAIXA (0)       0.80      0.85      0.83     55394\n",
      "    ALTA (1)       0.53      0.43      0.47     20826\n",
      "\n",
      "    accuracy                           0.74     76220\n",
      "   macro avg       0.66      0.64      0.65     76220\n",
      "weighted avg       0.73      0.74      0.73     76220\n",
      "\n",
      "\n",
      "Amostra dos Resultados (Transformer):\n",
      "             date    ticker  y_real  y_pred_proba  y_pred_class\n",
      "333265 2022-11-08  DXCO3.SA       0      0.257109             0\n",
      "333379 2022-11-09  DXCO3.SA       0      0.266457             0\n",
      "333424 2022-11-10  DXCO3.SA       0      0.220990             0\n",
      "333584 2022-11-11  DXCO3.SA       0      0.408827             0\n",
      "333651 2022-11-14  DXCO3.SA       0      0.414264             0\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Avaliação de Classificação\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previsões no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > THRESHOLD).astype(int)\n",
    "\n",
    "    # 3. Buscar as informações originais (data, ticker) usando os índices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avaliação de Métricas de Classificação\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcurácia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # --- CÁLCULO DE ROC AUC ADICIONADO ---\n",
    "    # (Usa as probabilidades, não as classes)\n",
    "    roc_auc = roc_auc_score(df_results['y_real'], df_results['y_pred_proba'])\n",
    "    print(f\"ROC AUC no Teste: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (Transformer):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b268dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\n",
      "Usando modelo treinado para prever a PROBABILIDADE de alta em 15 dias.\n",
      "Usando os últimos 30 dias de dados como entrada.\n",
      "==================================================\n",
      "\n",
      "Gerando previsões para 108 tickers...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\n",
      "       ticker last_data_date  predicted_proba_ALTA\n",
      "89   ENJU3.SA     2025-10-28              0.984000\n",
      "97   ASAI3.SA     2025-10-28              0.975520\n",
      "75   HAPV3.SA     2025-10-28              0.956848\n",
      "47   AMAR3.SA     2025-10-28              0.942791\n",
      "14   TUPY3.SA     2025-10-28              0.931915\n",
      "106  AZTE3.SA     2025-10-28              0.917421\n",
      "59   AZZA3.SA     2025-10-28              0.906454\n",
      "65   ANIM3.SA     2025-10-28              0.906327\n",
      "26   AZEV3.SA     2025-10-28              0.899902\n",
      "61   MGLU3.SA     2025-10-28              0.898159\n",
      "\n",
      "--- PIORES 10 (Menor Probabilidade de ALTA) ---\n",
      "      ticker last_data_date  predicted_proba_ALTA\n",
      "86  HBSA3.SA     2025-10-28              0.232841\n",
      "17  VIVT3.SA     2025-10-28              0.221055\n",
      "35  TIMS3.SA     2025-10-28              0.219978\n",
      "28  UGPA3.SA     2025-10-28              0.207611\n",
      "76  NEOE3.SA     2025-10-28              0.201036\n",
      "78  BMGB4.SA     2025-10-28              0.199983\n",
      "51  ENEV3.SA     2025-10-28              0.160502\n",
      "21  VALE3.SA     2025-10-28              0.154083\n",
      "50  BPAN4.SA     2025-10-28              0.146973\n",
      "8   ELET3.SA     2025-10-28              0.124592\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Geração de Recomendações (baseado em probabilidade)\n",
    "\n",
    "# --- SESSÃO DE PREVISÃO E RECOMENDAÇÃO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os últimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequências de entrada para a previsão\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequência\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as últimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar à lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previsões (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequências em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previsões para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previsão (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomendações\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequência válida pôde ser criada para previsão.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' não foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0539bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando cálculo de Permutation Feature Importance (usando ROC AUC)...\n",
      "Testando 81 features...\n",
      "Calculando ROC AUC base...\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "ROC AUC Base: 0.7301\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  1/81: close -> Drop: -0.0000 (9.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  2/81: high -> Drop: +0.0000 (8.9s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  3/81: low -> Drop: +0.0000 (9.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  4/81: open -> Drop: -0.0000 (9.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  5/81: volume -> Drop: -0.0000 (9.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  6/81: EMA_9 -> Drop: -0.0000 (9.1s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  7/81: SMA_21 -> Drop: -0.0000 (8.8s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  8/81: SMA_50 -> Drop: +0.0000 (8.7s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  9/81: SMA_200 -> Drop: +0.0000 (8.6s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  10/81: RSI_14 -> Drop: +0.0174 (8.7s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  11/81: MACD_12_26_9 -> Drop: -0.0000 (8.8s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  12/81: MACDh_12_26_9 -> Drop: +0.0000 (8.8s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  13/81: MACDs_12_26_9 -> Drop: +0.0000 (8.8s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  14/81: BBL_20_2.0_2.0 -> Drop: -0.0000 (8.7s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "  15/81: BBM_20_2.0_2.0 -> Drop: -0.0000 (8.9s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  16/81: BBU_20_2.0_2.0 -> Drop: +0.0000 (8.7s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  17/81: BBB_20_2.0_2.0 -> Drop: +0.0015 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  18/81: BBP_20_2.0_2.0 -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  19/81: STOCHk_14_3_3 -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  20/81: STOCHd_14_3_3 -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  21/81: STOCHh_14_3_3 -> Drop: +0.0000 (8.6s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  22/81: OBV -> Drop: +0.0114 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  23/81: ATRr_14 -> Drop: -0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  24/81: ADX_14 -> Drop: +0.0007 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  25/81: ADXR_14_2 -> Drop: -0.0004 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  26/81: DMP_14 -> Drop: +0.0011 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  27/81: DMN_14 -> Drop: -0.0022 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  28/81: close_sem -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  29/81: high_sem -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  30/81: low_sem -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  31/81: open_sem -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  32/81: volume_sem -> Drop: -0.0001 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  33/81: EMA_9_sem -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  34/81: SMA_21_sem -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  35/81: SMA_50_sem -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  36/81: SMA_200_sem -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  37/81: RSI_14_sem -> Drop: -0.0056 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  38/81: MACD_12_26_9_sem -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  39/81: MACDh_12_26_9_sem -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  40/81: MACDs_12_26_9_sem -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  41/81: BBL_20_2.0_2.0_sem -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  42/81: BBM_20_2.0_2.0_sem -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  43/81: BBU_20_2.0_2.0_sem -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  44/81: BBB_20_2.0_2.0_sem -> Drop: +0.0007 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  45/81: BBP_20_2.0_2.0_sem -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  46/81: STOCHk_14_3_3_sem -> Drop: -0.0040 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  47/81: STOCHd_14_3_3_sem -> Drop: -0.0034 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  48/81: STOCHh_14_3_3_sem -> Drop: +0.0126 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  49/81: OBV_sem -> Drop: +0.0180 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  50/81: ATRr_14_sem -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  51/81: ADX_14_sem -> Drop: +0.0075 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  52/81: ADXR_14_2_sem -> Drop: +0.0007 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  53/81: DMP_14_sem -> Drop: -0.0031 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  54/81: DMN_14_sem -> Drop: +0.0043 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  55/81: close_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  56/81: high_men -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  57/81: low_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  58/81: open_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  59/81: volume_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  60/81: EMA_9_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  61/81: SMA_21_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  62/81: SMA_50_men -> Drop: -0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  63/81: SMA_200_men -> Drop: +0.0083 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  64/81: RSI_14_men -> Drop: +0.1676 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  65/81: MACD_12_26_9_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  66/81: MACDh_12_26_9_men -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  67/81: MACDs_12_26_9_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  68/81: BBL_20_2.0_2.0_men -> Drop: +0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  69/81: BBM_20_2.0_2.0_men -> Drop: -0.0000 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  70/81: BBU_20_2.0_2.0_men -> Drop: +0.0000 (8.3s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  71/81: BBB_20_2.0_2.0_men -> Drop: +0.0014 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  72/81: BBP_20_2.0_2.0_men -> Drop: +0.0006 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  73/81: STOCHk_14_3_3_men -> Drop: +0.0289 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  74/81: STOCHd_14_3_3_men -> Drop: +0.0537 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  75/81: STOCHh_14_3_3_men -> Drop: +0.1106 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  76/81: OBV_men -> Drop: +0.0664 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  77/81: ATRr_14_men -> Drop: +0.0000 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  78/81: ADX_14_men -> Drop: +0.0087 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  79/81: ADXR_14_2_men -> Drop: +0.0169 (8.4s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  80/81: DMP_14_men -> Drop: +0.0282 (8.5s)\n",
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "  81/81: DMN_14_men -> Drop: +0.0045 (8.5s)\n",
      "\n",
      "--- TOP 15 Features Mais Importantes (baseado em ROC AUC) ---\n",
      "| feature           |   importance_drop |\n",
      "|:------------------|------------------:|\n",
      "| RSI_14_men        |        0.167636   |\n",
      "| STOCHh_14_3_3_men |        0.110643   |\n",
      "| OBV_men           |        0.0664481  |\n",
      "| STOCHd_14_3_3_men |        0.0536759  |\n",
      "| STOCHk_14_3_3_men |        0.0288775  |\n",
      "| DMP_14_men        |        0.0282403  |\n",
      "| OBV_sem           |        0.0180361  |\n",
      "| RSI_14            |        0.01742    |\n",
      "| ADXR_14_2_men     |        0.0169211  |\n",
      "| STOCHh_14_3_3_sem |        0.0125915  |\n",
      "| OBV               |        0.0114113  |\n",
      "| ADX_14_men        |        0.00865996 |\n",
      "| SMA_200_men       |        0.00829925 |\n",
      "| ADX_14_sem        |        0.00749905 |\n",
      "| DMN_14_men        |        0.00448343 |\n",
      "\n",
      "--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\n",
      "| feature           |   importance_drop |\n",
      "|:------------------|------------------:|\n",
      "| MACD_12_26_9_sem  |      -1.31281e-06 |\n",
      "| ATRr_14           |      -4.22707e-06 |\n",
      "| volume            |      -3.5483e-05  |\n",
      "| volume_sem        |      -5.10235e-05 |\n",
      "| ADXR_14_2         |      -0.000409326 |\n",
      "| DMN_14            |      -0.00222811  |\n",
      "| DMP_14_sem        |      -0.00309204  |\n",
      "| STOCHd_14_3_3_sem |      -0.00341441  |\n",
      "| STOCHk_14_3_3_sem |      -0.00395075  |\n",
      "| RSI_14_sem        |      -0.0055856   |\n",
      "\n",
      "--- Cálculo de Permutation Importance Concluído ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 0. Garanta que temos os dados e o modelo\n",
    "if 'model_transformer' in locals() and 'X_test_seq' in locals() and 'y_test_seq' in locals() and 'features' in locals():\n",
    "    \n",
    "    print(\"Iniciando cálculo de Permutation Feature Importance (usando ROC AUC)...\")\n",
    "    print(f\"Testando {len(features)} features...\")\n",
    "\n",
    "    # 1. Calcular o ROC AUC Base (Baseline)\n",
    "    print(\"Calculando ROC AUC base...\")\n",
    "    # Usamos as probabilidades (saída bruta do modelo)\n",
    "    y_pred_base_proba = model_transformer.predict(X_test_seq).flatten() \n",
    "    baseline_roc_auc = roc_auc_score(y_test_seq, y_pred_base_proba) # <-- MUDANÇA\n",
    "    print(f\"ROC AUC Base: {baseline_roc_auc:.4f}\") # <-- MUDANÇA\n",
    "\n",
    "    importances = []\n",
    "    \n",
    "    # 2. Loop por CADA feature\n",
    "    for i, feature_name in enumerate(features):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Criar uma cópia dos dados de teste\n",
    "        X_test_permuted = np.copy(X_test_seq)\n",
    "        \n",
    "        # 3. Embaralhar (permutar) os valores APENAS da feature 'i'\n",
    "        values_to_shuffle = X_test_permuted[:, :, i].flatten()\n",
    "        np.random.shuffle(values_to_shuffle)\n",
    "        X_test_permuted[:, :, i] = values_to_shuffle.reshape(\n",
    "            (X_test_seq.shape[0], X_test_seq.shape[1])\n",
    "        )\n",
    "        \n",
    "        # 4. Fazer novas previsões com os dados embaralhados\n",
    "        y_pred_permuted_proba = model_transformer.predict(X_test_permuted).flatten()\n",
    "        \n",
    "        # 5. Calcular o novo ROC AUC\n",
    "        permuted_roc_auc = roc_auc_score(y_test_seq, y_pred_permuted_proba) # <-- MUDANÇA\n",
    "        \n",
    "        # 6. Salvar a QUEDA de importância (na métrica ROC AUC)\n",
    "        importance_drop = baseline_roc_auc - permuted_roc_auc # <-- MUDANÇA\n",
    "        importances.append({\n",
    "            'feature': feature_name,\n",
    "            'importance_drop': importance_drop\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        # <-- MUDANÇA no print para mostrar melhor o drop do AUC\n",
    "        print(f\"  {i+1}/{len(features)}: {feature_name} -> Drop: {importance_drop:+.4f} ({(end_time-start_time):.1f}s)\")\n",
    "\n",
    "\n",
    "    # Converte a lista de resultados em um DataFrame e ordena pela importância\n",
    "    df_importances = pd.DataFrame(importances).sort_values(by='importance_drop', ascending=False)\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(\"\\n--- TOP 15 Features Mais Importantes (baseado em ROC AUC) ---\")\n",
    "    print(df_importances.head(15).to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\")\n",
    "    print(df_importances.tail(10).to_markdown(index=False))\n",
    "    \n",
    "    print(\"\\n--- Cálculo de Permutation Importance Concluído ---\")\n",
    "    # df_importances # O DataFrame será a saída da célula\n",
    "    \n",
    "else:\n",
    "    print(\"ERRO: Variáveis necessárias (modelo, X_test_seq, etc.) não encontradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features originais: 81\n",
      "Features restantes (v2): 55\n",
      "\n",
      "Features selecionadas (as únicas que ajudam o modelo):\n",
      "['RSI_14_men', 'STOCHh_14_3_3_men', 'OBV_men', 'STOCHd_14_3_3_men', 'STOCHk_14_3_3_men', 'DMP_14_men', 'OBV_sem', 'RSI_14', 'ADXR_14_2_men', 'STOCHh_14_3_3_sem', 'OBV', 'ADX_14_men', 'SMA_200_men', 'ADX_14_sem', 'DMN_14_men', 'DMN_14_sem', 'BBB_20_2.0_2.0', 'BBB_20_2.0_2.0_men', 'DMP_14', 'ADXR_14_2_sem', 'BBB_20_2.0_2.0_sem', 'ADX_14', 'BBP_20_2.0_2.0_men', 'MACD_12_26_9_men', 'MACDs_12_26_9_men', 'close_sem', 'ATRr_14_men', 'MACDh_12_26_9_men', 'SMA_200', 'SMA_200_sem', 'BBL_20_2.0_2.0_men', 'STOCHh_14_3_3', 'BBU_20_2.0_2.0', 'BBU_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'high', 'STOCHd_14_3_3', 'open_men', 'close_men', 'low', 'BBP_20_2.0_2.0', 'MACDh_12_26_9', 'open_sem', 'SMA_21_sem', 'BBU_20_2.0_2.0_men', 'MACDs_12_26_9_sem', 'EMA_9_men', 'volume_men', 'low_men', 'SMA_50', 'SMA_21_men', 'MACDs_12_26_9', 'high_sem', 'STOCHk_14_3_3', 'BBP_20_2.0_2.0_sem']\n"
     ]
    }
   ],
   "source": [
    "# Adicione esta nova célula\n",
    "# Filtra o DataFrame para pegar apenas features com importância positiva\n",
    "df_features_v2 = df_importances[df_importances['importance_drop'] > 0]\n",
    "\n",
    "# Cria a nova lista de features\n",
    "features_v2 = df_features_v2['feature'].tolist()\n",
    "\n",
    "print(f\"Features originais: {len(features)}\")\n",
    "print(f\"Features restantes (v2): {len(features_v2)}\")\n",
    "print(\"\\nFeatures selecionadas (as únicas que ajudam o modelo):\")\n",
    "print(features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- USANDO FEATURE SET V2 (OTIMIZADO) ---\n",
      "Usando 55 features selecionadas:\n",
      "['RSI_14_men', 'STOCHh_14_3_3_men', 'OBV_men', 'STOCHd_14_3_3_men', 'STOCHk_14_3_3_men', 'DMP_14_men', 'OBV_sem', 'RSI_14', 'ADXR_14_2_men', 'STOCHh_14_3_3_sem', 'OBV', 'ADX_14_men', 'SMA_200_men', 'ADX_14_sem', 'DMN_14_men', 'DMN_14_sem', 'BBB_20_2.0_2.0', 'BBB_20_2.0_2.0_men', 'DMP_14', 'ADXR_14_2_sem', 'BBB_20_2.0_2.0_sem', 'ADX_14', 'BBP_20_2.0_2.0_men', 'MACD_12_26_9_men', 'MACDs_12_26_9_men', 'close_sem', 'ATRr_14_men', 'MACDh_12_26_9_men', 'SMA_200', 'SMA_200_sem', 'BBL_20_2.0_2.0_men', 'STOCHh_14_3_3', 'BBU_20_2.0_2.0', 'BBU_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'high', 'STOCHd_14_3_3', 'open_men', 'close_men', 'low', 'BBP_20_2.0_2.0', 'MACDh_12_26_9', 'open_sem', 'SMA_21_sem', 'BBU_20_2.0_2.0_men', 'MACDs_12_26_9_sem', 'EMA_9_men', 'volume_men', 'low_men', 'SMA_50', 'SMA_21_men', 'MACDs_12_26_9', 'high_sem', 'STOCHk_14_3_3', 'BBP_20_2.0_2.0_sem']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Definição das Features (X) ---\n",
    "# V2 - Usando a seleção de features da Célula [11] para combater overfitting\n",
    "\n",
    "# A variável 'features' agora será esta lista otimizada\n",
    "features = features_v2\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final está vazio. Rode as células [2] e [3] primeiro.\")\n",
    "else:\n",
    "    # Garante que as colunas 'y' não estão em 'features'\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    features = [col for col in features if col not in colunas_a_excluir]\n",
    "\n",
    "    print(f\"--- USANDO FEATURE SET V2 (OTIMIZADO) ---\")\n",
    "    print(f\"Usando {len(features)} features selecionadas:\")\n",
    "    print(features)\n",
    "    \n",
    "    # A verificação de Leakage continua importante\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO: {leaky_cols} !!!\")\n",
    "    else:\n",
    "        print(\">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    \n",
    "    # Definir X e y para as células seguintes\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e99243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (254270, 55)\n",
      "Shape Validação 2D (X_validation): (63568, 55)\n",
      "Shape Teste 2D (X_test): (79460, 55)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2020-01-06 00:00:00\n",
      "Datas de Validação: 2020-01-06 00:00:00 a 2022-09-23 00:00:00\n",
      "Datas de Teste: 2022-09-23 00:00:00 a 2025-10-07 00:00:00\n",
      "\n",
      "X_train_scaled shape: (254270, 55)\n",
      "X_val_scaled shape: (63568, 55)\n",
      "X_test_scaled shape: (79460, 55)\n"
     ]
    }
   ],
   "source": [
    "# A Partir daqui refaz o modelo treinando as features mais relevantes\n",
    "# <-- MUDANÇA: Solução 2 - Split Treino/Validação/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+validação\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Validação (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Validação 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Validação: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normalização (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os três sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a função de sequência)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ec072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequências de TREINO...\n",
      "Criando sequências de VALIDAÇÃO...\n",
      "Criando sequências de TESTE...\n",
      "\n",
      "Formato das Sequências de Treino (X): (251907, 30, 55)\n",
      "Formato dos Alvos de Treino (y): (251907,)\n",
      "\n",
      "Formato das Sequências de Validação (X): (60458, 30, 55)\n",
      "Formato dos Alvos de Validação (y): (60458,)\n",
      "\n",
      "Formato das Sequências de Teste (X): (76220, 30, 55)\n",
      "Formato dos Alvos de Teste (y): (76220,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequências de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequências não cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os índices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker específico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O índice original é mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele será ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequência de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # Índice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDANÇA: Solução 2 - Criar Sequências para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequências de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de VALIDAÇÃO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta é a variável que a Célula [9] (Avaliação) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequências de Validação (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Validação (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste não encontrados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb1763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de Classe Calculados: {np.int64(0): np.float64(0.704847898106281), np.int64(1): np.float64(1.7204176967941975)}\n",
      "Reloading Tuner from keras_tuner_dir\\stock_TRANSFORMER_tuning_weighted_rocauc_featuresv2\\tuner0.json\n",
      "--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\n",
      "--- BUSCA CONCLUÍDA ---\n",
      "\n",
      "    Melhores hiperparâmetros encontrados:\n",
      "    - num_blocks: 1\n",
      "    - num_heads: 4\n",
      "    - head_size: 128\n",
      "    - ff_dim: 34\n",
      "    - dropout: 0.20\n",
      "    - dense_units: 64\n",
      "    - learning_rate: 0.00027\n",
      "    \n",
      "--- TREINANDO O MODELO FINAL COM OS MELHORES HPs ---\n",
      "Treinando o modelo final por 10 epochs.\n",
      "Epoch 1/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 16ms/step - accuracy: 0.5450 - loss: 0.6722 - roc_auc: 0.5972 - val_accuracy: 0.4922 - val_loss: 0.7240 - val_roc_auc: 0.5804\n",
      "Epoch 2/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 16ms/step - accuracy: 0.6149 - loss: 0.6287 - roc_auc: 0.6893 - val_accuracy: 0.5997 - val_loss: 0.6545 - val_roc_auc: 0.7043\n",
      "Epoch 3/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 16ms/step - accuracy: 0.6654 - loss: 0.5845 - roc_auc: 0.7497 - val_accuracy: 0.6354 - val_loss: 0.6272 - val_roc_auc: 0.7462\n",
      "Epoch 4/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 16ms/step - accuracy: 0.6862 - loss: 0.5634 - roc_auc: 0.7723 - val_accuracy: 0.6526 - val_loss: 0.6100 - val_roc_auc: 0.7592\n",
      "Epoch 5/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 16ms/step - accuracy: 0.6948 - loss: 0.5529 - roc_auc: 0.7830 - val_accuracy: 0.6248 - val_loss: 0.6442 - val_roc_auc: 0.7796\n",
      "Epoch 6/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 16ms/step - accuracy: 0.7008 - loss: 0.5459 - roc_auc: 0.7893 - val_accuracy: 0.6463 - val_loss: 0.6328 - val_roc_auc: 0.7643\n",
      "Epoch 7/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 16ms/step - accuracy: 0.7044 - loss: 0.5399 - roc_auc: 0.7948 - val_accuracy: 0.6111 - val_loss: 0.6997 - val_roc_auc: 0.7560\n",
      "Epoch 8/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 17ms/step - accuracy: 0.7069 - loss: 0.5355 - roc_auc: 0.7983 - val_accuracy: 0.6650 - val_loss: 0.6164 - val_roc_auc: 0.7588\n",
      "Epoch 9/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 17ms/step - accuracy: 0.7084 - loss: 0.5316 - roc_auc: 0.8016 - val_accuracy: 0.6229 - val_loss: 0.6713 - val_roc_auc: 0.7520\n",
      "Epoch 10/10\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 17ms/step - accuracy: 0.7125 - loss: 0.5279 - roc_auc: 0.8050 - val_accuracy: 0.6619 - val_loss: 0.6052 - val_roc_auc: 0.7670\n",
      "Modelo final (Transformer) treinado e carregado na variável 'model_transformer'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_Tuner_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Transformer_Tuner_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Input_Sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input_Sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">114,231</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,904</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,925</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Global_Pooling      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Global_Pooling[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense_Classifier    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ Dense_Classifier… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Input_Sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ Input_Sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │    \u001b[38;5;34m114,231\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │        \u001b[38;5;34m110\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m34\u001b[0m)    │      \u001b[38;5;34m1,904\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │      \u001b[38;5;34m1,925\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m55\u001b[0m)    │        \u001b[38;5;34m110\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Global_Pooling      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ Global_Pooling[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Dense_Classifier    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m3,584\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ Dense_Classifier… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">365,789</span> (1.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m365,789\u001b[0m (1.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,929</span> (476.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,929\u001b[0m (476.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,860</span> (952.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m243,860\u001b[0m (952.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando o melhor modelo em 'transformers_stock_model_30d_ts30_CLASS_weighted_featurev2.keras'...\n",
      "Modelo salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = 'transformers_stock_model_30d_ts30_CLASS_weighted_featurev2.keras'\n",
    "\n",
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um único bloco Transformer Encoder.\n",
    "    d_model = dimensão da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta atenção\" a diferentes partes da sequência de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimensão original\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Função Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Função construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimensões dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimensão do modelo é o número de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro não sabe a *ordem* dos dias (é permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprendível\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posições (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posições (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posição aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperparâmetros para Tunar ---\n",
    "    \n",
    "    # 1. Número de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Parâmetros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabeças\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimensão da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regularização)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=5e-4, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que você tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constrói a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabeça de Classificação (Classification Head) ---\n",
    "    \n",
    "    # A saída 'x' ainda é uma sequência (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um único vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a média dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classificação\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classificação (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[AUC(name='roc_auc', curve='ROC'), 'accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configuração do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes únicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores às classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicionário de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq não encontrado. Pulando cálculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados não estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective=kt.Objective(\"val_roc_auc\", direction=\"max\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted_rocauc_featuresv2'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLUÍDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperparâmetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperparâmetros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # SOLUÇÃO: RE-TREINAR O MELHOR MODELO\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"--- TREINANDO O MODELO FINAL COM OS MELHORES HPs ---\")\n",
    "\n",
    "    # 1. Construir o modelo final usando os melhores HPs\n",
    "    #    (best_hps já foi pego na etapa anterior)\n",
    "    model_transformer = build_model(best_hps)\n",
    "\n",
    "    # 2. (Opcional, mas recomendado) Recupere o número ideal de epochs\n",
    "    #    se o EarlyStopping parou o melhor \"trial\" mais cedo.\n",
    "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "    print(\"Buscando a melhor época do 'best_trial'...\")\n",
    "\n",
    "    # 1. Pegar o histórico da métrica que seu EarlyStopping monitora (val_loss)\n",
    "    #    Isso retorna uma lista de objetos: [MetricHistory(epoch=0, value=0.5), MetricHistory(epoch=1, value=0.45), ...]\n",
    "    val_loss_history_objects = best_trial.metrics.get_history(\"val_loss\")\n",
    "\n",
    "    # 2. Extrair APENAS os valores numéricos para o numpy\n",
    "    #    Isso cria uma lista como: [0.5, 0.45, 0.48]\n",
    "    val_loss_values = [m.value for m in val_loss_history_objects]\n",
    "\n",
    "    # 3. Encontrar o índice (época) que teve o *menor* valor de loss\n",
    "    #    Usamos np.argmin() porque \"loss\" deve ser minimizada.\n",
    "    best_epoch_index = np.argmin(val_loss_values)\n",
    "\n",
    "    # 4. Converter o índice (base-0) para o número de épocas (base-1)\n",
    "    #    Se o índice 0 for o melhor, treinamos por 1 época.\n",
    "    #    Se o índice 1 for o melhor, treinamos por 2 épocas.\n",
    "    best_epochs = best_epoch_index + 1\n",
    "\n",
    "    print(f\"Melhor época encontrada (baseada no val_loss): {best_epochs}\")\n",
    "\n",
    "    print(f\"Treinando o modelo final por {best_epochs} epochs.\")\n",
    "\n",
    "    # 3. Treinar o modelo final. \n",
    "    #    Desta vez, usamos os mesmos dados de treino E validação.\n",
    "    #    O EarlyStopping é usado para garantir que ele pare no ponto certo.\n",
    "    history = model_transformer.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_epochs, # Treina pelo número ideal de épocas\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping], # Use o mesmo EarlyStopping\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Modelo final (Transformer) treinado e carregado na variável 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # 4. Salvar o modelo final treinado\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' não foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cee21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2382/2382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "\n",
      "Acurácia (Accuracy) no Teste: 75.02%\n",
      "ROC AUC no Teste: 0.7628\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[47892  7502]\n",
      " [11536  9290]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BAIXA (0)       0.81      0.86      0.83     55394\n",
      "    ALTA (1)       0.55      0.45      0.49     20826\n",
      "\n",
      "    accuracy                           0.75     76220\n",
      "   macro avg       0.68      0.66      0.66     76220\n",
      "weighted avg       0.74      0.75      0.74     76220\n",
      "\n",
      "\n",
      "Amostra dos Resultados (Transformer):\n",
      "             date    ticker  y_real  y_pred_proba  y_pred_class\n",
      "333265 2022-11-08  DXCO3.SA       0      0.060416             0\n",
      "333379 2022-11-09  DXCO3.SA       0      0.047502             0\n",
      "333424 2022-11-10  DXCO3.SA       0      0.036787             0\n",
      "333584 2022-11-11  DXCO3.SA       0      0.166942             0\n",
      "333651 2022-11-14  DXCO3.SA       0      0.151635             0\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Avaliação de Classificação\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previsões no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > THRESHOLD).astype(int)\n",
    "\n",
    "    # 3. Buscar as informações originais (data, ticker) usando os índices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avaliação de Métricas de Classificação\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcurácia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # --- CÁLCULO DE ROC AUC ADICIONADO ---\n",
    "    # (Usa as probabilidades, não as classes)\n",
    "    roc_auc = roc_auc_score(df_results['y_real'], df_results['y_pred_proba'])\n",
    "    print(f\"ROC AUC no Teste: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (Transformer):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2da607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\n",
      "Usando modelo treinado para prever a PROBABILIDADE de alta em 15 dias.\n",
      "Usando os últimos 30 dias de dados como entrada.\n",
      "==================================================\n",
      "\n",
      "Gerando previsões para 108 tickers...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\n",
      "       ticker last_data_date  predicted_proba_ALTA\n",
      "75   HAPV3.SA     2025-10-28              0.991801\n",
      "79   LWSA3.SA     2025-10-28              0.990946\n",
      "103  AURE3.SA     2025-10-28              0.981291\n",
      "25   PCAR3.SA     2025-10-28              0.978823\n",
      "61   MGLU3.SA     2025-10-28              0.977187\n",
      "36   CSAN3.SA     2025-10-28              0.973861\n",
      "91   BRAV3.SA     2025-10-28              0.971672\n",
      "56   FLRY3.SA     2025-10-28              0.971361\n",
      "45   MRVE3.SA     2025-10-28              0.971322\n",
      "89   ENJU3.SA     2025-10-28              0.967704\n",
      "\n",
      "--- PIORES 10 (Menor Probabilidade de ALTA) ---\n",
      "      ticker last_data_date  predicted_proba_ALTA\n",
      "7   SBSP3.SA     2025-10-28              0.444288\n",
      "83  PGMN3.SA     2025-10-28              0.420440\n",
      "82  AMBP3.SA     2025-10-28              0.419600\n",
      "6   EMBR3.SA     2025-10-28              0.413181\n",
      "51  ENEV3.SA     2025-10-28              0.401258\n",
      "12  CPLE6.SA     2025-10-28              0.359441\n",
      "17  VIVT3.SA     2025-10-28              0.237906\n",
      "50  BPAN4.SA     2025-10-28              0.231382\n",
      "35  TIMS3.SA     2025-10-28              0.203996\n",
      "19  BRFS3.SA     2025-09-22              0.157285\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Geração de Recomendações (baseado em probabilidade)\n",
    "\n",
    "# --- SESSÃO DE PREVISÃO E RECOMENDAÇÃO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os últimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequências de entrada para a previsão\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequência\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as últimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar à lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previsões (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequências em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previsões para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previsão (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomendações\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequência válida pôde ser criada para previsão.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' não foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f293a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
