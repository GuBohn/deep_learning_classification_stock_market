{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ade6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75db9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Di√°rio: (298956, 29)\n",
      "Semanal: (63758, 29)\n",
      "Mensal: (14695, 29)\n",
      "\n",
      "Tipo da coluna 'datetime' (Di√°rio): datetime64[ns]\n",
      "\n",
      "Amostra Dados Di√°rios:\n",
      "    datetime     close      high       low      open       volume    ticker  \\\n",
      "0 2000-01-03  1.156394  1.156394  1.156394  1.156394  35389440000  PETR4.SA   \n",
      "1 2000-01-04  1.092423  1.092423  1.092423  1.092423  28861440000  PETR4.SA   \n",
      "2 2000-01-05  1.081401  1.081401  1.081401  1.081401  43033600000  PETR4.SA   \n",
      "3 2000-01-06  1.077661  1.077661  1.077661  1.077661  34055680000  PETR4.SA   \n",
      "4 2000-01-07  1.082582  1.082582  1.082582  1.082582  20912640000  PETR4.SA   \n",
      "\n",
      "   EMA_9  SMA_21  SMA_50  ...  BBP_20_2.0_2.0  STOCHk_14_3_3  STOCHd_14_3_3  \\\n",
      "0    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "1    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "2    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "3    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "4    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "\n",
      "   STOCHh_14_3_3           OBV  ATRr_14  ADX_14  ADXR_14_2  DMP_14  DMN_14  \n",
      "0            NaN           NaN      NaN     NaN        NaN     NaN     NaN  \n",
      "1            NaN -2.886144e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "2            NaN -7.189504e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "3            NaN -1.059507e+11      NaN     NaN        NaN     NaN     NaN  \n",
      "4            NaN -8.503808e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Conectar ao banco de dados SQLite\n",
    "DB_FILE = \"dados_acoes.db\"\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "\n",
    "# Carregar as tabelas\n",
    "# O usu√°rio confirmou que as colunas 'date' j√° est√£o em formato datetime\n",
    "try:\n",
    "    df_diario = pd.read_sql_query(\"SELECT * FROM diario_com_indicadores\", conn)\n",
    "    df_semanal = pd.read_sql_query(\"SELECT * FROM semanal_com_indicadores\", conn)\n",
    "    df_mensal = pd.read_sql_query(\"SELECT * FROM mensal_com_indicadores\", conn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Verifica√ß√£o extra: garantir que o pandas as reconheceu como datetime\n",
    "    # Se elas forem strings, o merge_asof falhar√°.\n",
    "    df_diario['datetime'] = pd.to_datetime(df_diario['datetime'])\n",
    "    df_semanal['datetime'] = pd.to_datetime(df_semanal['datetime'])\n",
    "    df_mensal['datetime'] = pd.to_datetime(df_mensal['datetime'])\n",
    "\n",
    "    print(f\"Di√°rio: {df_diario.shape}\")\n",
    "    print(f\"Semanal: {df_semanal.shape}\")\n",
    "    print(f\"Mensal: {df_mensal.shape}\")\n",
    "    \n",
    "    print(\"\\nTipo da coluna 'datetime' (Di√°rio):\", df_diario['datetime'].dtype)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    print(\"Verifique se o nome do arquivo 'dados_acoes.db' e os nomes das tabelas est√£o corretos.\")\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# Visualizar os dados\n",
    "print(\"\\nAmostra Dados Di√°rios:\")\n",
    "print(df_diario.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amostra de Dados Unificados:\n",
      "    datetime     close      high       low      open       volume    ticker  \\\n",
      "0 2000-01-03  1.156394  1.156394  1.156394  1.156394  35389440000  PETR4.SA   \n",
      "1 2000-01-03  1.451446  1.483197  1.443886  1.481684       571500  USIM5.SA   \n",
      "2 2000-01-03  1.144166  1.144166  1.144166  1.144166   3998720000  PETR3.SA   \n",
      "3 2000-01-03  4.192249  4.271596  4.172356  4.231811         2536  SBSP3.SA   \n",
      "4 2000-01-03  6.061346  6.565222  5.927966  6.565222      1121444  EMBR3.SA   \n",
      "\n",
      "       EMA_9     SMA_21     SMA_50  ...  BBP_20_2.0_2.0_men  \\\n",
      "0        NaN        NaN        NaN  ...                 NaN   \n",
      "1  25.765796  30.936735  32.537200  ...           -0.515013   \n",
      "2  12.106565  14.745436  15.046883  ...           -0.437854   \n",
      "3  67.605249  76.724392  77.451645  ...           -0.152599   \n",
      "4  34.396744  39.484826  39.348730  ...           -0.253103   \n",
      "\n",
      "   STOCHk_14_3_3_men  STOCHd_14_3_3_men  STOCHh_14_3_3_men       OBV_men  \\\n",
      "0                NaN                NaN                NaN           NaN   \n",
      "1          42.190079          46.436990          -4.246911  4.484260e+11   \n",
      "2          63.713640          78.761567         -15.047927  2.449538e+11   \n",
      "3          65.320627          82.266584         -16.945956  3.176196e+11   \n",
      "4          69.285274          83.640621         -14.355347  3.165633e+11   \n",
      "\n",
      "   ATRr_14_men  ADX_14_men  ADXR_14_2_men  DMP_14_men  DMN_14_men  \n",
      "0          NaN         NaN            NaN         NaN         NaN  \n",
      "1     8.042867   34.295418      31.993775    7.486907   33.101973  \n",
      "2     3.352578   19.427564      16.965609   12.077743   36.473645  \n",
      "3    18.902813   61.315526      63.577988   14.017000   27.820325  \n",
      "4     9.629089   18.542184      16.959022   14.943942   33.544670  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "Shape ap√≥s merge: (298956, 83)\n"
     ]
    }
   ],
   "source": [
    "# Garantir que tudo est√° ordenado por data para o merge_asof funcionar\n",
    "df_diario = df_diario.sort_values(by='datetime')\n",
    "df_semanal = df_semanal.sort_values(by='datetime')\n",
    "df_mensal = df_mensal.sort_values(by='datetime')\n",
    "\n",
    "# Renomear colunas de indicadores para evitar conflitos (ex: 'RSI' di√°rio, 'RSI' semanal)\n",
    "df_semanal = df_semanal.add_suffix('_sem')\n",
    "df_mensal = df_mensal.add_suffix('_men')\n",
    "\n",
    "# Renomear colunas de jun√ß√£o\n",
    "df_semanal = df_semanal.rename(columns={'datetime_sem': 'datetime', 'ticker_sem': 'ticker'})\n",
    "df_mensal = df_mensal.rename(columns={'datetime_men': 'datetime', 'ticker_men': 'ticker'})\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Jun√ß√£o (Merge)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Juntar Di√°rio com Semanal\n",
    "# Para cada 'ticker', vamos juntar a data di√°ria com a data semanal mais pr√≥xima (anterior ou igual)\n",
    "df_merged = pd.merge_asof(\n",
    "    df_diario,\n",
    "    df_semanal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward' # 'backward' pega o √∫ltimo dado semanal dispon√≠vel para aquele dia\n",
    ")\n",
    "\n",
    "# 2. Juntar o resultado com o Mensal\n",
    "df_merged = pd.merge_asof(\n",
    "    df_merged,\n",
    "    df_mensal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "print(\"\\nAmostra de Dados Unificados:\")\n",
    "print(df_merged.head())\n",
    "\n",
    "print(f\"\\nShape ap√≥s merge: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981b6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defini√ß√£o do Alvo (y) ---\n",
    "PERIOD_HORIZON = 30\n",
    "TIME_STEPS = 30 # Hiperpar√¢metro: quantos dias o LSTM vai \"olhar para tr√°s\"\n",
    "\n",
    "# <-- MUDAN√áA: Threshold para classifica√ß√£o bin√°ria (0 = retorno > 0%)\n",
    "CLASSIFICATION_THRESHOLD = 0.05\n",
    "\n",
    "# <-- MUDAN√áA: Novo nome de arquivo para o modelo de classifica√ß√£o\n",
    "MODEL_FILE = \"transformers_stock_model_30d_ts30_CLASS_weighted.keras\" # Nome do arquivo para salvar/carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b2d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape final ap√≥s limpeza e 'y_target': (265399, 86)\n",
      "Distribui√ß√£o do Alvo (y_target):\n",
      "y_target\n",
      "0    0.632033\n",
      "1    0.367967\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Usando 81 features:\n",
      "['close', 'high', 'low', 'open', 'volume', 'EMA_9', 'SMA_21', 'SMA_50', 'SMA_200', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_20_2.0_2.0', 'BBM_20_2.0_2.0', 'BBU_20_2.0_2.0', 'BBB_20_2.0_2.0', 'BBP_20_2.0_2.0', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'STOCHh_14_3_3', 'OBV', 'ATRr_14', 'ADX_14', 'ADXR_14_2', 'DMP_14', 'DMN_14', 'close_sem', 'high_sem', 'low_sem', 'open_sem', 'volume_sem', 'EMA_9_sem', 'SMA_21_sem', 'SMA_50_sem', 'SMA_200_sem', 'RSI_14_sem', 'MACD_12_26_9_sem', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'BBL_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'BBU_20_2.0_2.0_sem', 'BBB_20_2.0_2.0_sem', 'BBP_20_2.0_2.0_sem', 'STOCHk_14_3_3_sem', 'STOCHd_14_3_3_sem', 'STOCHh_14_3_3_sem', 'OBV_sem', 'ATRr_14_sem', 'ADX_14_sem', 'ADXR_14_2_sem', 'DMP_14_sem', 'DMN_14_sem', 'close_men', 'high_men', 'low_men', 'open_men', 'volume_men', 'EMA_9_men', 'SMA_21_men', 'SMA_50_men', 'SMA_200_men', 'RSI_14_men', 'MACD_12_26_9_men', 'MACDh_12_26_9_men', 'MACDs_12_26_9_men', 'BBL_20_2.0_2.0_men', 'BBM_20_2.0_2.0_men', 'BBU_20_2.0_2.0_men', 'BBB_20_2.0_2.0_men', 'BBP_20_2.0_2.0_men', 'STOCHk_14_3_3_men', 'STOCHd_14_3_3_men', 'STOCHh_14_3_3_men', 'OBV_men', 'ATRr_14_men', 'ADX_14_men', 'ADXR_14_2_men', 'DMP_14_men', 'DMN_14_men']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verifica√ß√£o de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por ticker para calcular o shift corretamente\n",
    "df_merged['close_future'] = df_merged.groupby('ticker')['close'].shift(-PERIOD_HORIZON)\n",
    "\n",
    "\n",
    "# 1. Calcular o retorno futuro (para refer√™ncia e para criar o alvo)\n",
    "df_merged['y_return'] = (df_merged['close_future'] / df_merged['close']) - 1\n",
    "\n",
    "# 2. Criar o alvo de CLASSIFICA√á√ÉO (1 se o retorno > threshold, 0 caso contr√°rio)\n",
    "df_merged['y_target'] = (df_merged['y_return'] > CLASSIFICATION_THRESHOLD).astype(int)\n",
    "\n",
    "# --- Limpeza ---\n",
    "# Remover dados onde n√£o pudemos calcular o alvo (os √∫ltimos N dias de cada ticker)\n",
    "# Tamb√©m remover quaisquer NaNs gerados pelos merges ou c√°lculos de indicadores\n",
    "df_final = df_merged.dropna()\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final est√° vazio ap√≥s o dropna().\")\n",
    "    print(\"Verifique seus dados de entrada, a l√≥gica de merge e o c√°lculo do 'y_target'.\")\n",
    "else:\n",
    "    print(f\"\\nShape final ap√≥s limpeza e 'y_target': {df_final.shape}\")\n",
    "    print(f\"Distribui√ß√£o do Alvo (y_target):\\n{df_final['y_target'].value_counts(normalize=True)}\")\n",
    "\n",
    "    # --- Defini√ß√£o das Features (X) ---\n",
    "    features_diarias = [col for col in df_final.columns if col not in ['datetime', 'ticker'] and not col.endswith('_sem') and not col.endswith('_men')]\n",
    "    features_semanais = [col for col in df_final.columns if col.endswith('_sem') and col not in ['date_sem', 'ticker_sem']]\n",
    "    features_mensais = [col for col in df_final.columns if col.endswith('_men') and col not in ['date_men', 'ticker_men']]\n",
    "    all_features = features_diarias + features_semanais + features_mensais\n",
    "    \n",
    "    # Defina as colunas que NUNCA devem ser features\n",
    "    # <-- MUDAN√áA: Adicionado 'y_return' √† exclus√£o\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    \n",
    "    # Filtra para garantir que s√£o num√©ricas E n√£o s√£o as colunas de exclus√£o\n",
    "    features_numericas = [col for col in all_features if pd.api.types.is_numeric_dtype(df_final[col])]\n",
    "    features = [col for col in features_numericas if col not in colunas_a_excluir]\n",
    "    print(f\"\\nUsando {len(features)} features:\")\n",
    "    print(features)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO !!!\")\n",
    "        print(f\"As seguintes colunas-alvo EST√ÉO na sua lista 'features' (X): {leaky_cols}\")\n",
    "        print(f\"Corrija sua lista 'colunas_a_excluir' na C√©lula [5].\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    else:\n",
    "        print(\">>> Verifica√ß√£o de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7465383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (169855, 81)\n",
      "Shape Valida√ß√£o 2D (X_validation): (42464, 81)\n",
      "Shape Teste 2D (X_test): (53080, 81)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2019-10-31 00:00:00\n",
      "Datas de Valida√ß√£o: 2019-10-31 00:00:00 a 2022-07-29 00:00:00\n",
      "Datas de Teste: 2022-07-29 00:00:00 a 2025-09-12 00:00:00\n",
      "\n",
      "X_train_scaled shape: (169855, 81)\n",
      "X_val_scaled shape: (42464, 81)\n",
      "X_test_scaled shape: (53080, 81)\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDAN√áA: Solu√ß√£o 2 - Split Treino/Valida√ß√£o/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+valida√ß√£o\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Valida√ß√£o (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Valida√ß√£o 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Valida√ß√£o: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normaliza√ß√£o (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os tr√™s sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a fun√ß√£o de sequ√™ncia)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5639e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequ√™ncias de TREINO...\n",
      "Criando sequ√™ncias de VALIDA√á√ÉO...\n",
      "Criando sequ√™ncias de TESTE...\n",
      "\n",
      "Formato das Sequ√™ncias de Treino (X): (168160, 30, 81)\n",
      "Formato dos Alvos de Treino (y): (168160,)\n",
      "\n",
      "Formato das Sequ√™ncias de Valida√ß√£o (X): (40364, 30, 81)\n",
      "Formato dos Alvos de Valida√ß√£o (y): (40364,)\n",
      "\n",
      "Formato das Sequ√™ncias de Teste (X): (50950, 30, 81)\n",
      "Formato dos Alvos de Teste (y): (50950,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequ√™ncias de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequ√™ncias n√£o cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os √≠ndices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker espec√≠fico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O √≠ndice original √© mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele ser√° ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequ√™ncia de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # √çndice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDAN√áA: Solu√ß√£o 2 - Criar Sequ√™ncias para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequ√™ncias de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequ√™ncias de VALIDA√á√ÉO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequ√™ncias de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta √© a vari√°vel que a C√©lula [9] (Avalia√ß√£o) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequ√™ncias de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequ√™ncias de Valida√ß√£o (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Valida√ß√£o (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequ√™ncias de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste n√£o encontrados. Rode as c√©lulas anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c230f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 04m 52s]\n",
      "val_accuracy: 0.5846298933029175\n",
      "\n",
      "Best val_accuracy So Far: 0.5874789357185364\n",
      "Total elapsed time: 00h 08m 20s\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |1                 |num_blocks\n",
      "8                 |2                 |num_heads\n",
      "64                |96                |head_size\n",
      "324               |324               |ff_dim\n",
      "0.2               |0.2               |dropout\n",
      "0.0028389         |0.00044217        |learning_rate\n",
      "32                |48                |dense_units\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "3                 |3                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m 364/2628\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m3:32\u001b[0m 94ms/step - accuracy: 0.5139 - loss: 0.7150"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- INICIANDO BUSCA POR HIPERPAR√ÇMETROS (TRANSFORMER) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Iniciar a busca\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- BUSCA CONCLU√çDA ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# 1. Pegar os melhores hiperpar√¢metros\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[39m, in \u001b[36mBaseTuner.search\u001b[39m\u001b[34m(self, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_begin(trial)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_end(trial)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.on_search_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[39m, in \u001b[36mBaseTuner._try_run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m         trial.status = trial_module.TrialStatus.COMPLETED\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[39m, in \u001b[36mBaseTuner._run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.oracle.objective.name\n\u001b[32m    242\u001b[39m     ):\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[32m    245\u001b[39m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[32m    246\u001b[39m         warnings.warn(\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe use case of calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    255\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\tuners\\hyperband.py:427\u001b[39m, in \u001b[36mHyperband.run_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/epochs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    426\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33minitial_epoch\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/initial_epoch\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[39m, in \u001b[36mTuner.run_trial\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     callbacks.append(model_checkpoint)\n\u001b[32m    313\u001b[39m     copied_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m] = callbacks\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     obj_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     histories.append(obj_value)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[39m, in \u001b[36mTuner._build_and_fit_model\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hp = trial.hyperparameters\n\u001b[32m    232\u001b[39m model = \u001b[38;5;28mself\u001b[39m._try_build(hp)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.config.multi_backend():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[39m, in \u001b[36mHyperModel.fit\u001b[39m\u001b[34m(self, hp, model, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, *args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um √∫nico bloco Transformer Encoder.\n",
    "    d_model = dimens√£o da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta aten√ß√£o\" a diferentes partes da sequ√™ncia de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conex√£o Residual e Normaliza√ß√£o\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimens√£o original\n",
    "    \n",
    "    # Conex√£o Residual e Normaliza√ß√£o\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimens√µes dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimens√£o do modelo √© o n√∫mero de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro n√£o sabe a *ordem* dos dias (√© permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprend√≠vel\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posi√ß√µes (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posi√ß√µes (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posi√ß√£o aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperpar√¢metros para Tunar ---\n",
    "    \n",
    "    # 1. N√∫mero de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Par√¢metros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabe√ßas\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimens√£o da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model * 2, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regulariza√ß√£o)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.4, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que voc√™ tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constr√≥i a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabe√ßa de Classifica√ß√£o (Classification Head) ---\n",
    "    \n",
    "    # A sa√≠da 'x' ainda √© uma sequ√™ncia (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um √∫nico vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a m√©dia dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classifica√ß√£o\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classifica√ß√£o (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configura√ß√£o do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes √∫nicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores √†s classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicion√°rio de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq n√£o encontrado. Pulando c√°lculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados n√£o estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPAR√ÇMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLU√çDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperpar√¢metros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperpar√¢metros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Pegar o melhor modelo\n",
    "    model_transformer = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    print(\"Melhor modelo (Transformer) carregado na vari√°vel 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # Salvar o melhor modelo\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' n√£o foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- MUDAN√áA: Solu√ß√£o 1 - Avalia√ß√£o de Classifica√ß√£o\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previs√µes no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # 3. Buscar as informa√ß√µes originais (data, ticker) usando os √≠ndices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avalia√ß√£o de M√©tricas de Classifica√ß√£o\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcur√°cia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nMatriz de Confus√£o:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (LSTM):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b268dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- MUDAN√áA: Solu√ß√£o 1 - Gera√ß√£o de Recomenda√ß√µes (baseado em probabilidade)\n",
    "\n",
    "# --- SESS√ÉO DE PREVIS√ÉO E RECOMENDA√á√ÉO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERA√á√ÉO DE RECOMENDA√á√ïES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os √∫ltimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequ√™ncias de entrada para a previs√£o\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequ√™ncia\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as √∫ltimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar √† lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previs√µes (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequ√™ncias em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previs√µes para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previs√£o (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomenda√ß√µes\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDA√á√ïES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequ√™ncia v√°lida p√¥de ser criada para previs√£o.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' n√£o foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0539bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 0. Garanta que temos os dados e o modelo\n",
    "if 'model_transformer' in locals() and 'X_test_seq' in locals() and 'y_test_seq' in locals() and 'features' in locals():\n",
    "    \n",
    "    print(\"Iniciando c√°lculo de Permutation Feature Importance...\")\n",
    "    print(f\"Testando {len(features)} features...\")\n",
    "\n",
    "    # 1. Calcular a Acur√°cia Base (Baseline)\n",
    "    print(\"Calculando acur√°cia base...\")\n",
    "    y_pred_base_proba = model_transformer.predict(X_test_seq)\n",
    "    y_pred_base_class = (y_pred_base_proba > 0.5).astype(int)\n",
    "    baseline_accuracy = accuracy_score(y_test_seq, y_pred_base_class)\n",
    "    print(f\"Acur√°cia Base: {baseline_accuracy * 100:.2f}%\")\n",
    "\n",
    "    importances = []\n",
    "    \n",
    "    # 2. Loop por CADA feature\n",
    "    for i, feature_name in enumerate(features):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Criar uma c√≥pia dos dados de teste\n",
    "        X_test_permuted = np.copy(X_test_seq)\n",
    "        \n",
    "        # 3. Embaralhar (permutar) os valores APENAS da feature 'i'\n",
    "        # X_test_permuted[:, :, i] √© um \"slice\" 2D (amostras, timesteps)\n",
    "        # Vamos embaralhar os valores dentro desse slice\n",
    "        \n",
    "        # Pegar todos os valores da feature 'i'\n",
    "        values_to_shuffle = X_test_permuted[:, :, i].flatten()\n",
    "        # Embaralhar\n",
    "        np.random.shuffle(values_to_shuffle)\n",
    "        # Colocar de volta no array\n",
    "        X_test_permuted[:, :, i] = values_to_shuffle.reshape(\n",
    "            (X_test_seq.shape[0], X_test_seq.shape[1])\n",
    "        )\n",
    "        \n",
    "        # 4. Fazer novas previs√µes com os dados embaralhados\n",
    "        y_pred_permuted_proba = model_transformer.predict(X_test_permuted)\n",
    "        y_pred_permuted_class = (y_pred_permuted_proba > 0.5).astype(int)\n",
    "        \n",
    "        # 5. Calcular a nova acur√°cia\n",
    "        permuted_accuracy = accuracy_score(y_test_seq, y_pred_permuted_class)\n",
    "        \n",
    "        # 6. Salvar a QUEDA de import√¢ncia\n",
    "        importance_drop = baseline_accuracy - permuted_accuracy\n",
    "        importances.append({\n",
    "            'feature': feature_name,\n",
    "            'importance_drop': importance_drop\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"  {i+1}/{len(features)}: {feature_name} -> Drop: {importance_drop*100:+.2f}% ({(end_time-start_time):.1f}s)\")\n",
    "\n",
    "\n",
    "    # Converte a lista de resultados em um DataFrame e ordena pela import√¢ncia\n",
    "    df_importances = pd.DataFrame(importances).sort_values(by='importance_drop', ascending=False)\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(\"\\n--- TOP 15 Features Mais Importantes ---\")\n",
    "    print(df_importances.head(15).to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\")\n",
    "    print(df_importances.tail(10).to_markdown(index=False))\n",
    "    # 7. Mostrar os resultados\n",
    "    print(\"\\n--- C√°lculo de Permutation Importance Conclu√≠do ---\")\n",
    "    df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione esta nova c√©lula\n",
    "# Filtra o DataFrame para pegar apenas features com import√¢ncia positiva\n",
    "df_features_v2 = df_importances[df_importances['importance_drop'] > 0]\n",
    "\n",
    "# Cria a nova lista de features\n",
    "features_v2 = df_features_v2['feature'].tolist()\n",
    "\n",
    "print(f\"Features originais: {len(features)}\")\n",
    "print(f\"Features restantes (v2): {len(features_v2)}\")\n",
    "print(\"\\nFeatures selecionadas (as √∫nicas que ajudam o modelo):\")\n",
    "print(features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e99243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
