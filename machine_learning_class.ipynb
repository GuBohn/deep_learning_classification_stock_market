{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ade6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75db9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diário: (298956, 29)\n",
      "Semanal: (63758, 29)\n",
      "Mensal: (14695, 29)\n",
      "\n",
      "Tipo da coluna 'datetime' (Diário): datetime64[ns]\n",
      "\n",
      "Amostra Dados Diários:\n",
      "    datetime     close      high       low      open       volume    ticker  \\\n",
      "0 2000-01-03  1.156394  1.156394  1.156394  1.156394  35389440000  PETR4.SA   \n",
      "1 2000-01-04  1.092423  1.092423  1.092423  1.092423  28861440000  PETR4.SA   \n",
      "2 2000-01-05  1.081401  1.081401  1.081401  1.081401  43033600000  PETR4.SA   \n",
      "3 2000-01-06  1.077661  1.077661  1.077661  1.077661  34055680000  PETR4.SA   \n",
      "4 2000-01-07  1.082582  1.082582  1.082582  1.082582  20912640000  PETR4.SA   \n",
      "\n",
      "   EMA_9  SMA_21  SMA_50  ...  BBP_20_2.0_2.0  STOCHk_14_3_3  STOCHd_14_3_3  \\\n",
      "0    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "1    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "2    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "3    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "4    NaN     NaN     NaN  ...             NaN            NaN            NaN   \n",
      "\n",
      "   STOCHh_14_3_3           OBV  ATRr_14  ADX_14  ADXR_14_2  DMP_14  DMN_14  \n",
      "0            NaN           NaN      NaN     NaN        NaN     NaN     NaN  \n",
      "1            NaN -2.886144e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "2            NaN -7.189504e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "3            NaN -1.059507e+11      NaN     NaN        NaN     NaN     NaN  \n",
      "4            NaN -8.503808e+10      NaN     NaN        NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Conectar ao banco de dados SQLite\n",
    "DB_FILE = \"dados_acoes.db\"\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "\n",
    "# Carregar as tabelas\n",
    "# O usuário confirmou que as colunas 'date' já estão em formato datetime\n",
    "try:\n",
    "    df_diario = pd.read_sql_query(\"SELECT * FROM diario_com_indicadores\", conn)\n",
    "    df_semanal = pd.read_sql_query(\"SELECT * FROM semanal_com_indicadores\", conn)\n",
    "    df_mensal = pd.read_sql_query(\"SELECT * FROM mensal_com_indicadores\", conn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Verificação extra: garantir que o pandas as reconheceu como datetime\n",
    "    # Se elas forem strings, o merge_asof falhará.\n",
    "    df_diario['datetime'] = pd.to_datetime(df_diario['datetime'])\n",
    "    df_semanal['datetime'] = pd.to_datetime(df_semanal['datetime'])\n",
    "    df_mensal['datetime'] = pd.to_datetime(df_mensal['datetime'])\n",
    "\n",
    "    print(f\"Diário: {df_diario.shape}\")\n",
    "    print(f\"Semanal: {df_semanal.shape}\")\n",
    "    print(f\"Mensal: {df_mensal.shape}\")\n",
    "    \n",
    "    print(\"\\nTipo da coluna 'datetime' (Diário):\", df_diario['datetime'].dtype)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    print(\"Verifique se o nome do arquivo 'dados_acoes.db' e os nomes das tabelas estão corretos.\")\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# Visualizar os dados\n",
    "print(\"\\nAmostra Dados Diários:\")\n",
    "print(df_diario.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amostra de Dados Unificados:\n",
      "    datetime     close      high       low      open       volume    ticker  \\\n",
      "0 2000-01-03  1.156394  1.156394  1.156394  1.156394  35389440000  PETR4.SA   \n",
      "1 2000-01-03  1.451446  1.483197  1.443886  1.481684       571500  USIM5.SA   \n",
      "2 2000-01-03  1.144166  1.144166  1.144166  1.144166   3998720000  PETR3.SA   \n",
      "3 2000-01-03  4.192249  4.271596  4.172356  4.231811         2536  SBSP3.SA   \n",
      "4 2000-01-03  6.061346  6.565222  5.927966  6.565222      1121444  EMBR3.SA   \n",
      "\n",
      "       EMA_9     SMA_21     SMA_50  ...  BBP_20_2.0_2.0_men  \\\n",
      "0        NaN        NaN        NaN  ...                 NaN   \n",
      "1  25.765796  30.936735  32.537200  ...           -0.515013   \n",
      "2  12.106565  14.745436  15.046883  ...           -0.437854   \n",
      "3  67.605249  76.724392  77.451645  ...           -0.152599   \n",
      "4  34.396744  39.484826  39.348730  ...           -0.253103   \n",
      "\n",
      "   STOCHk_14_3_3_men  STOCHd_14_3_3_men  STOCHh_14_3_3_men       OBV_men  \\\n",
      "0                NaN                NaN                NaN           NaN   \n",
      "1          42.190079          46.436990          -4.246911  4.484260e+11   \n",
      "2          63.713640          78.761567         -15.047927  2.449538e+11   \n",
      "3          65.320627          82.266584         -16.945956  3.176196e+11   \n",
      "4          69.285274          83.640621         -14.355347  3.165633e+11   \n",
      "\n",
      "   ATRr_14_men  ADX_14_men  ADXR_14_2_men  DMP_14_men  DMN_14_men  \n",
      "0          NaN         NaN            NaN         NaN         NaN  \n",
      "1     8.042867   34.295418      31.993775    7.486907   33.101973  \n",
      "2     3.352578   19.427564      16.965609   12.077743   36.473645  \n",
      "3    18.902813   61.315526      63.577988   14.017000   27.820325  \n",
      "4     9.629089   18.542184      16.959022   14.943942   33.544670  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "Shape após merge: (298956, 83)\n"
     ]
    }
   ],
   "source": [
    "# Garantir que tudo está ordenado por data para o merge_asof funcionar\n",
    "df_diario = df_diario.sort_values(by='datetime')\n",
    "df_semanal = df_semanal.sort_values(by='datetime')\n",
    "df_mensal = df_mensal.sort_values(by='datetime')\n",
    "\n",
    "# Renomear colunas de indicadores para evitar conflitos (ex: 'RSI' diário, 'RSI' semanal)\n",
    "df_semanal = df_semanal.add_suffix('_sem')\n",
    "df_mensal = df_mensal.add_suffix('_men')\n",
    "\n",
    "# Renomear colunas de junção\n",
    "df_semanal = df_semanal.rename(columns={'datetime_sem': 'datetime', 'ticker_sem': 'ticker'})\n",
    "df_mensal = df_mensal.rename(columns={'datetime_men': 'datetime', 'ticker_men': 'ticker'})\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Junção (Merge)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Juntar Diário com Semanal\n",
    "# Para cada 'ticker', vamos juntar a data diária com a data semanal mais próxima (anterior ou igual)\n",
    "df_merged = pd.merge_asof(\n",
    "    df_diario,\n",
    "    df_semanal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward' # 'backward' pega o último dado semanal disponível para aquele dia\n",
    ")\n",
    "\n",
    "# 2. Juntar o resultado com o Mensal\n",
    "df_merged = pd.merge_asof(\n",
    "    df_merged,\n",
    "    df_mensal,\n",
    "    on='datetime',\n",
    "    by='ticker',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "print(\"\\nAmostra de Dados Unificados:\")\n",
    "print(df_merged.head())\n",
    "\n",
    "print(f\"\\nShape após merge: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981b6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definição do Alvo (y) ---\n",
    "PERIOD_HORIZON = 30\n",
    "TIME_STEPS = 30 # Hiperparâmetro: quantos dias o LSTM vai \"olhar para trás\"\n",
    "\n",
    "# <-- MUDANÇA: Threshold para classificação binária (0 = retorno > 0%)\n",
    "CLASSIFICATION_THRESHOLD = 0.05\n",
    "\n",
    "# <-- MUDANÇA: Novo nome de arquivo para o modelo de classificação\n",
    "MODEL_FILE = \"transformers_stock_model_30d_ts30_CLASS_weighted.keras\" # Nome do arquivo para salvar/carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b2d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape final após limpeza e 'y_target': (265399, 86)\n",
      "Distribuição do Alvo (y_target):\n",
      "y_target\n",
      "0    0.632033\n",
      "1    0.367967\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Usando 81 features:\n",
      "['close', 'high', 'low', 'open', 'volume', 'EMA_9', 'SMA_21', 'SMA_50', 'SMA_200', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'BBL_20_2.0_2.0', 'BBM_20_2.0_2.0', 'BBU_20_2.0_2.0', 'BBB_20_2.0_2.0', 'BBP_20_2.0_2.0', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'STOCHh_14_3_3', 'OBV', 'ATRr_14', 'ADX_14', 'ADXR_14_2', 'DMP_14', 'DMN_14', 'close_sem', 'high_sem', 'low_sem', 'open_sem', 'volume_sem', 'EMA_9_sem', 'SMA_21_sem', 'SMA_50_sem', 'SMA_200_sem', 'RSI_14_sem', 'MACD_12_26_9_sem', 'MACDh_12_26_9_sem', 'MACDs_12_26_9_sem', 'BBL_20_2.0_2.0_sem', 'BBM_20_2.0_2.0_sem', 'BBU_20_2.0_2.0_sem', 'BBB_20_2.0_2.0_sem', 'BBP_20_2.0_2.0_sem', 'STOCHk_14_3_3_sem', 'STOCHd_14_3_3_sem', 'STOCHh_14_3_3_sem', 'OBV_sem', 'ATRr_14_sem', 'ADX_14_sem', 'ADXR_14_2_sem', 'DMP_14_sem', 'DMN_14_sem', 'close_men', 'high_men', 'low_men', 'open_men', 'volume_men', 'EMA_9_men', 'SMA_21_men', 'SMA_50_men', 'SMA_200_men', 'RSI_14_men', 'MACD_12_26_9_men', 'MACDh_12_26_9_men', 'MACDs_12_26_9_men', 'BBL_20_2.0_2.0_men', 'BBM_20_2.0_2.0_men', 'BBU_20_2.0_2.0_men', 'BBB_20_2.0_2.0_men', 'BBP_20_2.0_2.0_men', 'STOCHk_14_3_3_men', 'STOCHd_14_3_3_men', 'STOCHh_14_3_3_men', 'OBV_men', 'ATRr_14_men', 'ADX_14_men', 'ADXR_14_2_men', 'DMP_14_men', 'DMN_14_men']\n",
      "\n",
      "--- Verificando Vazamento de Dados (Leakage) ---\n",
      ">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por ticker para calcular o shift corretamente\n",
    "df_merged['close_future'] = df_merged.groupby('ticker')['close'].shift(-PERIOD_HORIZON)\n",
    "\n",
    "\n",
    "# 1. Calcular o retorno futuro (para referência e para criar o alvo)\n",
    "df_merged['y_return'] = (df_merged['close_future'] / df_merged['close']) - 1\n",
    "\n",
    "# 2. Criar o alvo de CLASSIFICAÇÃO (1 se o retorno > threshold, 0 caso contrário)\n",
    "df_merged['y_target'] = (df_merged['y_return'] > CLASSIFICATION_THRESHOLD).astype(int)\n",
    "\n",
    "# --- Limpeza ---\n",
    "# Remover dados onde não pudemos calcular o alvo (os últimos N dias de cada ticker)\n",
    "# Também remover quaisquer NaNs gerados pelos merges ou cálculos de indicadores\n",
    "df_final = df_merged.dropna()\n",
    "\n",
    "if df_final.empty:\n",
    "    print(\"ERRO: O DataFrame final está vazio após o dropna().\")\n",
    "    print(\"Verifique seus dados de entrada, a lógica de merge e o cálculo do 'y_target'.\")\n",
    "else:\n",
    "    print(f\"\\nShape final após limpeza e 'y_target': {df_final.shape}\")\n",
    "    print(f\"Distribuição do Alvo (y_target):\\n{df_final['y_target'].value_counts(normalize=True)}\")\n",
    "\n",
    "    # --- Definição das Features (X) ---\n",
    "    features_diarias = [col for col in df_final.columns if col not in ['datetime', 'ticker'] and not col.endswith('_sem') and not col.endswith('_men')]\n",
    "    features_semanais = [col for col in df_final.columns if col.endswith('_sem') and col not in ['date_sem', 'ticker_sem']]\n",
    "    features_mensais = [col for col in df_final.columns if col.endswith('_men') and col not in ['date_men', 'ticker_men']]\n",
    "    all_features = features_diarias + features_semanais + features_mensais\n",
    "    \n",
    "    # Defina as colunas que NUNCA devem ser features\n",
    "    # <-- MUDANÇA: Adicionado 'y_return' à exclusão\n",
    "    colunas_a_excluir = ['datetime', 'ticker', 'close_future', 'y_return', 'y_target']\n",
    "    \n",
    "    # Filtra para garantir que são numéricas E não são as colunas de exclusão\n",
    "    features_numericas = [col for col in all_features if pd.api.types.is_numeric_dtype(df_final[col])]\n",
    "    features = [col for col in features_numericas if col not in colunas_a_excluir]\n",
    "    print(f\"\\nUsando {len(features)} features:\")\n",
    "    print(features)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Verificando Vazamento de Dados (Leakage) ---\")\n",
    "    leaky_cols = [col for col in features if col in ['close_future', 'y_return', 'y_target']]\n",
    "\n",
    "    if len(leaky_cols) > 0:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ALERTA DE VAZAMENTO (LEAKAGE) DETECTADO !!!\")\n",
    "        print(f\"As seguintes colunas-alvo ESTÃO na sua lista 'features' (X): {leaky_cols}\")\n",
    "        print(f\"Corrija sua lista 'colunas_a_excluir' na Célula [5].\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    else:\n",
    "        print(\">>> Verificação de leakage OK. Nenhuma coluna-alvo encontrada em X.\\n\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    X = df_final[features]\n",
    "    y = df_final['y_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7465383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Treino 2D (X_train): (169855, 81)\n",
      "Shape Validação 2D (X_validation): (42464, 81)\n",
      "Shape Teste 2D (X_test): (53080, 81)\n",
      "Datas de Treino: 2000-01-03 00:00:00 a 2019-10-31 00:00:00\n",
      "Datas de Validação: 2019-10-31 00:00:00 a 2022-07-29 00:00:00\n",
      "Datas de Teste: 2022-07-29 00:00:00 a 2025-09-12 00:00:00\n",
      "\n",
      "X_train_scaled shape: (169855, 81)\n",
      "X_val_scaled shape: (42464, 81)\n",
      "X_test_scaled shape: (53080, 81)\n"
     ]
    }
   ],
   "source": [
    "# <-- MUDANÇA: Solução 2 - Split Treino/Validação/Teste Temporal\n",
    "\n",
    "if not df_final.empty:\n",
    "    # 1. Separar Teste (20% finais)\n",
    "    split_test = int(len(df_final) * 0.8)\n",
    "    df_train_val = df_final.iloc[:split_test] # 80% para treino+validação\n",
    "    df_test = df_final.iloc[split_test:]      # 20% para teste final\n",
    "\n",
    "    # 2. Separar Treino e Validação (dos 80% iniciais)\n",
    "    split_val = int(len(df_train_val) * 0.8)\n",
    "    df_train = df_train_val.iloc[:split_val] # 80% do df_train_val\n",
    "    df_validation = df_train_val.iloc[split_val:] # 20% do df_train_val\n",
    "    \n",
    "    # 3. Separar X, y, e tickers para cada set\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['y_target']\n",
    "    ticker_train = df_train['ticker']\n",
    "    \n",
    "    X_validation = df_validation[features]\n",
    "    y_validation = df_validation['y_target']\n",
    "    ticker_validation = df_validation['ticker']\n",
    "    \n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['y_target']\n",
    "    ticker_test = df_test['ticker']\n",
    "    \n",
    "    print(f\"Shape Treino 2D (X_train): {X_train.shape}\")\n",
    "    print(f\"Shape Validação 2D (X_validation): {X_validation.shape}\")\n",
    "    print(f\"Shape Teste 2D (X_test): {X_test.shape}\")\n",
    "    print(f\"Datas de Treino: {df_train['datetime'].min()} a {df_train['datetime'].max()}\")\n",
    "    print(f\"Datas de Validação: {df_validation['datetime'].min()} a {df_validation['datetime'].max()}\")\n",
    "    print(f\"Datas de Teste: {df_test['datetime'].min()} a {df_test['datetime'].max()}\")\n",
    "\n",
    "    # --- Normalização (Scaling) CORRETA ---\n",
    "    # 4. Instanciar o Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 5. Fit (Ajustar) o scaler APENAS nos dados de TREINO (X_train)\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # 6. Transformar (Aplicar) em TODOS os três sets\n",
    "    X_train_scaled_array = scaler.transform(X_train)\n",
    "    X_val_scaled_array = scaler.transform(X_validation)\n",
    "    X_test_scaled_array = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Criar DataFrames com os dados escalados (para a função de sequência)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=features, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=features, index=X_validation.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=features, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "    print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vazio, pulando etapas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5639e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando sequências de TREINO...\n",
      "Criando sequências de VALIDAÇÃO...\n",
      "Criando sequências de TESTE...\n",
      "\n",
      "Formato das Sequências de Treino (X): (168160, 30, 81)\n",
      "Formato dos Alvos de Treino (y): (168160,)\n",
      "\n",
      "Formato das Sequências de Validação (X): (40364, 30, 81)\n",
      "Formato dos Alvos de Validação (y): (40364,)\n",
      "\n",
      "Formato das Sequências de Teste (X): (50950, 30, 81)\n",
      "Formato dos Alvos de Teste (y): (50950,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences_por_ticker(X_data, y_data, tickers, time_steps):\n",
    "    \"\"\"\n",
    "    Cria sequências de dados 3D (para LSTM) agrupadas por ticker.\n",
    "    Garante que as sequências não cruzem tickers diferentes.\n",
    "    \"\"\"\n",
    "    all_X_seq, all_y_seq, all_indices = [], [], []\n",
    "    \n",
    "    # Usamos os índices originais de df_final para rastrear datas/tickers\n",
    "    unique_tickers = tickers.unique()\n",
    "    \n",
    "    for i, ticker in enumerate(unique_tickers):\n",
    "        # Filtrar dados para este ticker específico\n",
    "        ticker_mask = (tickers == ticker)\n",
    "        X_ticker = X_data[ticker_mask]\n",
    "        y_ticker = y_data[ticker_mask]\n",
    "        \n",
    "        # O índice original é mantido\n",
    "        ticker_indices = y_ticker.index\n",
    "        \n",
    "        # print(f\"Processando Ticker: {ticker} ({i+1}/{len(unique_tickers)}) - {len(X_ticker)} amostras\") # Debug\n",
    "        \n",
    "        # Aplicar a janela deslizante (sliding window) apenas neste ticker\n",
    "        # Se o ticker tem menos dados que 'time_steps', ele será ignorado\n",
    "        for j in range(len(X_ticker) - time_steps):\n",
    "            # Sequência de features (ex: dias 0 a 29)\n",
    "            seq = X_ticker.iloc[j:(j + time_steps)].values\n",
    "            \n",
    "            # Alvo (ex: dia 30)\n",
    "            target = y_ticker.iloc[j + time_steps]\n",
    "            \n",
    "            # Índice do alvo (para rastrear data/ticker depois)\n",
    "            target_index = ticker_indices[j + time_steps]\n",
    "            \n",
    "            all_X_seq.append(seq)\n",
    "            all_y_seq.append(target)\n",
    "            all_indices.append(target_index)\n",
    "            \n",
    "    return np.array(all_X_seq), np.array(all_y_seq), np.array(all_indices)\n",
    "\n",
    "# <-- MUDANÇA: Solução 2 - Criar Sequências para os 3 sets\n",
    "if 'X_train_scaled' in locals():\n",
    "    \n",
    "    print(\"Criando sequências de TREINO...\")\n",
    "    X_train_seq, y_train_seq, seq_indices_train = create_sequences_por_ticker(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        ticker_train, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de VALIDAÇÃO...\")\n",
    "    X_val_seq, y_val_seq, seq_indices_val = create_sequences_por_ticker(\n",
    "        X_val_scaled, \n",
    "        y_validation, \n",
    "        ticker_validation, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    print(\"Criando sequências de TESTE...\")\n",
    "    X_test_seq, y_test_seq, seq_indices_test = create_sequences_por_ticker(\n",
    "        X_test_scaled, \n",
    "        y_test, \n",
    "        ticker_test, \n",
    "        TIME_STEPS\n",
    "    )\n",
    "    \n",
    "    # Esta é a variável que a Célula [9] (Avaliação) usa\n",
    "    indices_test = seq_indices_test\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Treino (X): {X_train_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Treino (y): {y_train_seq.shape}\")\n",
    "\n",
    "    print(f\"\\nFormato das Sequências de Validação (X): {X_val_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Validação (y): {y_val_seq.shape}\")\n",
    "    \n",
    "    print(f\"\\nFormato das Sequências de Teste (X): {X_test_seq.shape}\")\n",
    "    print(f\"Formato dos Alvos de Teste (y): {y_test_seq.shape}\")\n",
    "else:\n",
    "    print(\"Dados de treino/teste não encontrados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c230f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 04m 52s]\n",
      "val_accuracy: 0.5846298933029175\n",
      "\n",
      "Best val_accuracy So Far: 0.5874789357185364\n",
      "Total elapsed time: 00h 08m 20s\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |1                 |num_blocks\n",
      "8                 |2                 |num_heads\n",
      "64                |96                |head_size\n",
      "324               |324               |ff_dim\n",
      "0.2               |0.2               |dropout\n",
      "0.0028389         |0.00044217        |learning_rate\n",
      "32                |48                |dense_units\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "3                 |3                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m 364/2628\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:32\u001b[0m 94ms/step - accuracy: 0.5139 - loss: 0.7150"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Iniciar a busca\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- BUSCA CONCLUÍDA ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# 1. Pegar os melhores hiperparâmetros\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[39m, in \u001b[36mBaseTuner.search\u001b[39m\u001b[34m(self, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_begin(trial)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_end(trial)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.on_search_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[39m, in \u001b[36mBaseTuner._try_run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m         trial.status = trial_module.TrialStatus.COMPLETED\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[39m, in \u001b[36mBaseTuner._run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.oracle.objective.name\n\u001b[32m    242\u001b[39m     ):\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[32m    245\u001b[39m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[32m    246\u001b[39m         warnings.warn(\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe use case of calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    255\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\tuners\\hyperband.py:427\u001b[39m, in \u001b[36mHyperband.run_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/epochs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    426\u001b[39m     fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33minitial_epoch\u001b[39m\u001b[33m\"\u001b[39m] = hp.values[\u001b[33m\"\u001b[39m\u001b[33mtuner/initial_epoch\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[39m, in \u001b[36mTuner.run_trial\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     callbacks.append(model_checkpoint)\n\u001b[32m    313\u001b[39m     copied_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m] = callbacks\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     obj_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     histories.append(obj_value)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[39m, in \u001b[36mTuner._build_and_fit_model\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hp = trial.hyperparameters\n\u001b[32m    232\u001b[39m model = \u001b[38;5;28mself\u001b[39m._try_build(hp)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.config.multi_backend():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[39m, in \u001b[36mHyperModel.fit\u001b[39m\u001b[34m(self, hp, model, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, *args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gustavo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Helper Function: Bloco Transformer Encoder ---\n",
    "def transformer_encoder_block(inputs, d_model, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Cria um único bloco Transformer Encoder.\n",
    "    d_model = dimensão da feature (no nosso caso, n_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Multi-Head Attention (Self-Attention) ---\n",
    "    # O modelo \"presta atenção\" a diferentes partes da sequência de 30 dias\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs) # Query=inputs, Key=inputs, Value=inputs (self-attention)\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # --- 2. Feed Forward Network ---\n",
    "    # Uma rede neural simples aplicada a cada \"dia\" (time step)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(d_model)(ffn_output) # Projeta de volta para a dimensão original\n",
    "    \n",
    "    # Conexão Residual e Normalização\n",
    "    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# --- Função Construtora de Modelo (para KerasTuner) ---\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Função construtora de modelo para o KerasTuner, usando Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegar as dimensões dos dados de treino\n",
    "    n_samples, time_steps, n_features = X_train_seq.shape\n",
    "    d_model = n_features # Dimensão do modelo é o número de features\n",
    "    \n",
    "    # --- Input ---\n",
    "    inputs = layers.Input(shape=(time_steps, d_model), name=\"Input_Sequence\")\n",
    "    x = inputs\n",
    "    \n",
    "    # --- 1. Positional Embedding ---\n",
    "    # O Transformer puro não sabe a *ordem* dos dias (é permutation-invariant).\n",
    "    # Precisamos adicionar uma \"Positional Embedding\" para que ele saiba \n",
    "    # qual dia veio antes de qual. Usamos uma Embedding \"aprendível\".\n",
    "    \n",
    "    # Cria uma camada de embedding para as posições (0, 1, ..., 29)\n",
    "    pos_embedding_layer = layers.Embedding(input_dim=time_steps, output_dim=d_model, name=\"PositionalEmbedding\")\n",
    "    # Cria as posições (constante)\n",
    "    positions = tf.range(start=0, limit=time_steps, delta=1)\n",
    "    # Adiciona o embedding da posição aos dados de entrada\n",
    "    x = x + pos_embedding_layer(positions)\n",
    "    \n",
    "    \n",
    "    # --- Hiperparâmetros para Tunar ---\n",
    "    \n",
    "    # 1. Número de blocos Transformer para empilhar\n",
    "    hp_num_blocks = hp.Int('num_blocks', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    # 2. Parâmetros da Multi-Head Attention\n",
    "    hp_num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2) # Ex: 2, 4, 8 cabeças\n",
    "    hp_head_size = hp.Int('head_size', min_value=32, max_value=128, step=32) # key_dim\n",
    "    \n",
    "    # 3. Dimensão da camada Feed-Forward interna\n",
    "    hp_ff_dim = hp.Int('ff_dim', min_value=d_model * 2, max_value=d_model * 4, step=d_model) # Ex: 128, 256\n",
    "    \n",
    "    # 4. Dropout (para regularização)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.4, step=0.1)\n",
    "    \n",
    "    # 5. Learning Rate\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # 6. Unidades da camada Densa (igual ao que você tinha)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32, max_value=64, step=16)\n",
    "    # ------------------------------------\n",
    "\n",
    "    # --- 2. Stack de Encoders ---\n",
    "    # Constrói a arquitetura empilhando os blocos\n",
    "    for _ in range(hp_num_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            inputs=x,\n",
    "            d_model=d_model,\n",
    "            head_size=hp_head_size,\n",
    "            num_heads=hp_num_heads,\n",
    "            ff_dim=hp_ff_dim,\n",
    "            dropout_rate=hp_dropout\n",
    "        )\n",
    "\n",
    "    # --- 3. Cabeça de Classificação (Classification Head) ---\n",
    "    \n",
    "    # A saída 'x' ainda é uma sequência (Batch, 30, 81).\n",
    "    # Precisamos agregar tudo em um único vetor por amostra.\n",
    "    # GlobalAveragePooling1D tira a média dos 30 time steps.\n",
    "    x = layers.GlobalAveragePooling1D(name=\"Global_Pooling\")(x)\n",
    "    \n",
    "    # Camada Densa final para classificação\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_dense_units, activation='relu', name=\"Dense_Classifier\")(x)\n",
    "    \n",
    "    # Camada final de classificação (sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "\n",
    "    # --- 4. Compilar o Modelo ---\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Tuner_Model\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Configuração do KerasTuner ---\n",
    "\n",
    "\n",
    "# Verificar se y_train_seq existe antes de calcular os pesos\n",
    "if 'y_train_seq' in locals() and y_train_seq.size > 0:\n",
    "    # Obter as classes únicas (ex: [0, 1])\n",
    "    classes = np.unique(y_train_seq)\n",
    "    \n",
    "    # Calcular os pesos no modo 'balanced'\n",
    "    # Isso atribui pesos maiores às classes menos frequentes\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train_seq\n",
    "    )\n",
    "    \n",
    "    # Criar o dicionário de pesos que o Keras espera\n",
    "    # Ex: {0: 0.89, 1: 1.14}\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"Pesos de Classe Calculados: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"y_train_seq não encontrado. Pulando cálculo de pesos.\")\n",
    "    class_weight_dict = None # Definir como None se os dados não estiverem prontos\n",
    "\n",
    "\n",
    "if 'X_train_seq' in locals():\n",
    "    # Instanciar o Tuner. \n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='stock_TRANSFORMER_tuning_weighted'\n",
    "    )\n",
    "\n",
    "    # Callback de EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"--- INICIANDO BUSCA POR HIPERPARÂMETROS (TRANSFORMER) ---\")\n",
    "    \n",
    "    # Iniciar a busca\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "    print(\"--- BUSCA CONCLUÍDA ---\")\n",
    "\n",
    "    # 1. Pegar os melhores hiperparâmetros\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Melhores hiperparâmetros encontrados:\n",
    "    - num_blocks: {best_hps.get('num_blocks')}\n",
    "    - num_heads: {best_hps.get('num_heads')}\n",
    "    - head_size: {best_hps.get('head_size')}\n",
    "    - ff_dim: {best_hps.get('ff_dim')}\n",
    "    - dropout: {best_hps.get('dropout'):.2f}\n",
    "    - dense_units: {best_hps.get('dense_units')}\n",
    "    - learning_rate: {best_hps.get('learning_rate'):.5f}\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Pegar o melhor modelo\n",
    "    model_transformer = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    print(\"Melhor modelo (Transformer) carregado na variável 'model_transformer'.\")\n",
    "    model_transformer.summary()\n",
    "\n",
    "    # Salvar o melhor modelo\n",
    "    print(f\"Salvando o melhor modelo em '{MODEL_FILE}'...\")\n",
    "    model_transformer.save(MODEL_FILE)\n",
    "    print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "else:\n",
    "    print(\"AVISO: 'X_train_seq' não foi definido. Pulando hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2de976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Avaliação de Classificação\n",
    "\n",
    "if 'model_transformer' in locals():\n",
    "    # 1. Fazer previsões no set de teste (retorna probabilidades)\n",
    "    y_pred_proba = model_transformer.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # 2. Converter probabilidades em classes (ex: 0.5 como threshold)\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # 3. Buscar as informações originais (data, ticker) usando os índices salvos\n",
    "    test_info = df_final.loc[indices_test]\n",
    "\n",
    "    # 4. Criar o DataFrame de resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'date': test_info['datetime'],\n",
    "        'ticker': test_info['ticker'],\n",
    "        'y_real': y_test_seq,        # Alvos reais (0 ou 1)\n",
    "        'y_pred_proba': y_pred_proba,  # Probabilidade prevista (ex: 0.75)\n",
    "        'y_pred_class': y_pred_class   # Classe prevista (0 ou 1)\n",
    "    })\n",
    "\n",
    "    # 5. Avaliação de Métricas de Classificação\n",
    "    accuracy = accuracy_score(df_results['y_real'], df_results['y_pred_class'])\n",
    "    print(f\"\\nAcurácia (Accuracy) no Teste: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    # (Linhas = Real, Colunas = Previsto)\n",
    "    print(confusion_matrix(df_results['y_real'], df_results['y_pred_class']))\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(df_results['y_real'], df_results['y_pred_class'], target_names=['BAIXA (0)', 'ALTA (1)']))\n",
    "\n",
    "    print(\"\\nAmostra dos Resultados (LSTM):\")\n",
    "    print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b268dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- MUDANÇA: Solução 1 - Geração de Recomendações (baseado em probabilidade)\n",
    "\n",
    "# --- SESSÃO DE PREVISÃO E RECOMENDAÇÃO ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO GERAÇÃO DE RECOMENDAÇÕES FUTURAS\")\n",
    "print(f\"Usando modelo treinado para prever a PROBABILIDADE de alta em {PERIOD_HORIZON} dias.\")\n",
    "print(f\"Usando os últimos {TIME_STEPS} dias de dados como entrada.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'model_transformer' in locals() and 'scaler' in locals() and 'features' in locals():\n",
    "    \n",
    "    # 1. Preparar os dados mais recentes (do df_merged, antes do dropna)\n",
    "    # Queremos todas as linhas que tenham os indicadores (features) completos.\n",
    "    df_predict_input = df_merged.dropna(subset=features)\n",
    "    \n",
    "    # 2. Aplicar o Scaler (o mesmo que foi treinado)\n",
    "    X_predict_scaled_array = scaler.transform(df_predict_input[features])\n",
    "    \n",
    "    # Recriar o DataFrame com os dados escalados\n",
    "    X_predict_scaled = pd.DataFrame(\n",
    "        X_predict_scaled_array, \n",
    "        columns=features, \n",
    "        index=df_predict_input.index\n",
    "    )\n",
    "    \n",
    "    # Adicionar de volta o ticker e a data para podermos agrupar\n",
    "    X_predict_scaled['ticker'] = df_predict_input['ticker']\n",
    "    X_predict_scaled['datetime'] = df_predict_input['datetime']\n",
    "\n",
    "    # 3. Montar as sequências de entrada para a previsão\n",
    "    prediction_sequences = []\n",
    "    tickers_for_prediction = []\n",
    "    last_dates = []\n",
    "    \n",
    "    unique_tickers = X_predict_scaled['ticker'].unique()\n",
    "    \n",
    "    for ticker in unique_tickers:\n",
    "        # Pegar os dados do ticker e ordenar pela data\n",
    "        ticker_data = X_predict_scaled[\n",
    "            X_predict_scaled['ticker'] == ticker\n",
    "        ].sort_values(by='datetime')\n",
    "        \n",
    "        # Verificar se temos dados suficientes para uma sequência\n",
    "        if len(ticker_data) >= TIME_STEPS:\n",
    "            # Pegar as últimas TIME_STEPS linhas\n",
    "            last_sequence = ticker_data[features].iloc[-TIME_STEPS:].values\n",
    "            \n",
    "            # Adicionar à lista\n",
    "            prediction_sequences.append(last_sequence)\n",
    "            tickers_for_prediction.append(ticker)\n",
    "            last_dates.append(ticker_data['datetime'].iloc[-1])\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} ignorado (dados insuficientes: {len(ticker_data)} < {TIME_STEPS})\")\n",
    "\n",
    "    # 4. Fazer as Previsões (Probabilidades)\n",
    "    if len(prediction_sequences) > 0:\n",
    "        # Converter a lista de sequências em um array 3D numpy\n",
    "        X_to_predict = np.array(prediction_sequences)\n",
    "        print(f\"\\nGerando previsões para {X_to_predict.shape[0]} tickers...\")\n",
    "        \n",
    "        # Fazer a previsão (retorna probabilidades)\n",
    "        future_predictions_proba = model_transformer.predict(X_to_predict).flatten()\n",
    "        \n",
    "        # 5. Criar e Rankear o DataFrame de Recomendações\n",
    "        df_recommendations = pd.DataFrame({\n",
    "            'ticker': tickers_for_prediction,\n",
    "            'last_data_date': last_dates,\n",
    "            'predicted_proba_ALTA': future_predictions_proba\n",
    "        })\n",
    "        \n",
    "        # Ordenar pelas maiores probabilidades de ALTA\n",
    "        df_recommendations = df_recommendations.sort_values(\n",
    "            by='predicted_proba_ALTA', \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- TOP 10 RECOMENDAÇÕES (Maior Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.head(10))\n",
    "        \n",
    "        print(\"\\n--- PIORES 10 (Menor Probabilidade de ALTA) ---\")\n",
    "        print(df_recommendations.tail(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"Nenhuma sequência válida pôde ser criada para previsão.\")\n",
    "else:\n",
    "    print(\"ERRO: 'model_transformer' ou 'scaler' não foram encontrados.\")\n",
    "    print(\"Certifique-se de que o modelo foi treinado com sucesso antes de rodar esta etapa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0539bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 0. Garanta que temos os dados e o modelo\n",
    "if 'model_transformer' in locals() and 'X_test_seq' in locals() and 'y_test_seq' in locals() and 'features' in locals():\n",
    "    \n",
    "    print(\"Iniciando cálculo de Permutation Feature Importance...\")\n",
    "    print(f\"Testando {len(features)} features...\")\n",
    "\n",
    "    # 1. Calcular a Acurácia Base (Baseline)\n",
    "    print(\"Calculando acurácia base...\")\n",
    "    y_pred_base_proba = model_transformer.predict(X_test_seq)\n",
    "    y_pred_base_class = (y_pred_base_proba > 0.5).astype(int)\n",
    "    baseline_accuracy = accuracy_score(y_test_seq, y_pred_base_class)\n",
    "    print(f\"Acurácia Base: {baseline_accuracy * 100:.2f}%\")\n",
    "\n",
    "    importances = []\n",
    "    \n",
    "    # 2. Loop por CADA feature\n",
    "    for i, feature_name in enumerate(features):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Criar uma cópia dos dados de teste\n",
    "        X_test_permuted = np.copy(X_test_seq)\n",
    "        \n",
    "        # 3. Embaralhar (permutar) os valores APENAS da feature 'i'\n",
    "        # X_test_permuted[:, :, i] é um \"slice\" 2D (amostras, timesteps)\n",
    "        # Vamos embaralhar os valores dentro desse slice\n",
    "        \n",
    "        # Pegar todos os valores da feature 'i'\n",
    "        values_to_shuffle = X_test_permuted[:, :, i].flatten()\n",
    "        # Embaralhar\n",
    "        np.random.shuffle(values_to_shuffle)\n",
    "        # Colocar de volta no array\n",
    "        X_test_permuted[:, :, i] = values_to_shuffle.reshape(\n",
    "            (X_test_seq.shape[0], X_test_seq.shape[1])\n",
    "        )\n",
    "        \n",
    "        # 4. Fazer novas previsões com os dados embaralhados\n",
    "        y_pred_permuted_proba = model_transformer.predict(X_test_permuted)\n",
    "        y_pred_permuted_class = (y_pred_permuted_proba > 0.5).astype(int)\n",
    "        \n",
    "        # 5. Calcular a nova acurácia\n",
    "        permuted_accuracy = accuracy_score(y_test_seq, y_pred_permuted_class)\n",
    "        \n",
    "        # 6. Salvar a QUEDA de importância\n",
    "        importance_drop = baseline_accuracy - permuted_accuracy\n",
    "        importances.append({\n",
    "            'feature': feature_name,\n",
    "            'importance_drop': importance_drop\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"  {i+1}/{len(features)}: {feature_name} -> Drop: {importance_drop*100:+.2f}% ({(end_time-start_time):.1f}s)\")\n",
    "\n",
    "\n",
    "    # Converte a lista de resultados em um DataFrame e ordena pela importância\n",
    "    df_importances = pd.DataFrame(importances).sort_values(by='importance_drop', ascending=False)\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(\"\\n--- TOP 15 Features Mais Importantes ---\")\n",
    "    print(df_importances.head(15).to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- TOP 10 Features Menos Importantes (ou Prejudiciais) ---\")\n",
    "    print(df_importances.tail(10).to_markdown(index=False))\n",
    "    # 7. Mostrar os resultados\n",
    "    print(\"\\n--- Cálculo de Permutation Importance Concluído ---\")\n",
    "    df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione esta nova célula\n",
    "# Filtra o DataFrame para pegar apenas features com importância positiva\n",
    "df_features_v2 = df_importances[df_importances['importance_drop'] > 0]\n",
    "\n",
    "# Cria a nova lista de features\n",
    "features_v2 = df_features_v2['feature'].tolist()\n",
    "\n",
    "print(f\"Features originais: {len(features)}\")\n",
    "print(f\"Features restantes (v2): {len(features_v2)}\")\n",
    "print(\"\\nFeatures selecionadas (as únicas que ajudam o modelo):\")\n",
    "print(features_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e99243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
